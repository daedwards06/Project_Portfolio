{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>PROJECT 2</b></h2></center><center><h3><b>MSDS 6371 - 403</b></h3></center><center><h3><b>Data Mining</b></h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Team Members:</b></h4>\n",
    "<ul>\n",
    "  <li>Lisa Mendez</li>\n",
    "  <li>Brandon Lawrence</li>\n",
    "  <li>Mariana Llamas-Cendon</li>\n",
    "  <li>Dominique Edwards</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b><center> Data Preparation (15 points total)</center></b></h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "The notebook must be run from the beginning to ensure all of the variable values hold their value in order for the cells to run appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>[10 points]</b>\n",
    "\n",
    "*Define and prepare your class variables. Use proper variable\n",
    "representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for\n",
    "dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for\n",
    "the analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this project we are using the cleaned version of the data set ('accident.xlsx') we have derived for Project1 and used in the Mini-Lab, because we have already deleted those attributes that had more than 50% of missing or unknown values, plus some other attributes that we considered did not provide any useful information for our analysis. \n",
    "\n",
    "For this project our intention is to classify and determine which elements directly contribute to the probability that a death (or several deaths) will happen in an accident--for Task 1--, and the hour frame in which an accident having fatalities involved is more likely to occur--for Task 2--and based on this we will try to reduce noise caused by other attributes that we believe do not contribute to the analysis. We will also create two different dataframes that directly relate to the tasks we will be performing: a) predicting single or multiple fatalities in an accident; b) predicting the hour in which fatalities occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "#import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>ST_CASE</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>PVH_INVL</th>\n",
       "      <th>PEDS</th>\n",
       "      <th>PERNOTMVIT</th>\n",
       "      <th>PERMVIT</th>\n",
       "      <th>PERSONS</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>...</th>\n",
       "      <th>RELJCT1</th>\n",
       "      <th>RELJCT2</th>\n",
       "      <th>TYP_INT</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "      <th>REL_ROAD</th>\n",
       "      <th>LGT_COND</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>FATALS</th>\n",
       "      <th>DRUNK_DR</th>\n",
       "      <th>DATETIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>127.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10005</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  ST_CASE  VE_TOTAL  VE_FORMS  PVH_INVL  PEDS  PERNOTMVIT  PERMVIT  \\\n",
       "0      1    10001         1         1         0     0           0        1   \n",
       "1      1    10002         1         1         0     0           0        1   \n",
       "2      1    10003         1         1         0     0           0        2   \n",
       "3      1    10004         1         1         0     0           0        1   \n",
       "4      1    10005         2         2         0     0           0        2   \n",
       "\n",
       "   PERSONS  COUNTY    ...      RELJCT1  RELJCT2  TYP_INT  WRK_ZONE  REL_ROAD  \\\n",
       "0        1   127.0    ...          0.0      1.0      1.0         0       4.0   \n",
       "1        1    83.0    ...          0.0      1.0      1.0         0       3.0   \n",
       "2        2    11.0    ...          0.0      1.0      1.0         0       4.0   \n",
       "3        1    45.0    ...          0.0      1.0      1.0         0       4.0   \n",
       "4        2    45.0    ...          0.0      2.0      3.0         0       1.0   \n",
       "\n",
       "   LGT_COND  WEATHER FATALS  DRUNK_DR   DATETIME  \n",
       "0       2.0      1.0      1         1 2015-01-01  \n",
       "1       2.0     10.0      1         0 2015-01-01  \n",
       "2       2.0      1.0      1         1 2015-01-01  \n",
       "3       2.0     10.0      1         1 2015-01-04  \n",
       "4       1.0      1.0      1         0 2015-01-07  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accident = pd.read_excel('accident_clean.xlsx')\n",
    "df_accident.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of rows is       ', 30828)\n",
      "('The number of attributes is ', 33)\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_accident.shape[0])\n",
    "print ('The number of attributes is ', df_accident.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deletion of variables not relevant to analysis\n",
    "\n",
    "As mentioned prior, we will focus this analysis on those attributes that directly impact our response variable **FATALS**--which later will be used to derive a new response variable named **FATALITIES** which will only contain two classes: 0, for single fatality accidents and 1, for multiple deaths in  motor vehicle accidents. \n",
    "\n",
    "In this context, in which are main interest resides in understanding the conditions that could allow us to predict the probability in which an accident can result in one or multiple fatalities, we decided to drop attributes that a) can be derived from other attributes, and b) do not provide information regarding what could have caused someone to die in the event of an accident. These attributes are:\n",
    "\n",
    "<li>**ST_CASE**   *--a unique identifier for each report of an accident ( this does not contain information regarding whether fatalities occur);*</li>\n",
    "<li>**COUNTY**    *--represents the identifying codes of each of the counties where an accident took place, this attribute could allow us to discern those counties in which fatalities occur but cannot help us to determine the cause and therefore predict the likelihood of a death or deaths in an accident. *</li>\n",
    "<li>**SP_JUR**    *--identifies if the location on the trafficway where the crash occurred qualifies as a Special Jurisdiction or not. Considering that there are about 30,560 instances of class 0 (No Special Jurisdiction), this attribute only adds noise and not much information as information about this can be obtained from attribute **ROUTE**.*</li>\n",
    "<li>**RD_OWNER**  *-- This attribute is pretty much similar to that named **ROUTE**, only that in here the route is identified with a particular agency, whether Federal, State, or Local*.</li>\n",
    "<li>**RUR_URB**   *-- this attribute identifies if the road where the accident had ocurred is rural or urban, but that information can be derived from the attribute **ROUTE** *. \n",
    "<li>**LONGITUD**  *--This element identifies the location of the crash using Global Position coordinates. At this point we are more interested in where and how a death in an crash occur than knowing the exact coordinates where it happened.*<li>\n",
    "<li>**LATITUDE**  *--same as above.*</li>\n",
    "<li>**STATE**     *-- For this particular analysis we are interested in determining what factors contribute in people dying during an accident, not so much in comparing where it happened.*</li>\n",
    "<li>**FUNC_SYS**  *--This element describe the type of road where an accident happened. Thi information can be obtained from **ROUTE**.</li>\n",
    "<li> **TYP_INT**  *--This data element identifies and allows separation of various intersection types but can derived from **RELJCT2**, which identifies the crash's location with respect to presence in or\n",
    "proximity to components typically in junction or interchange areas.</li>\n",
    "<li> **NHS**      *--This data element identifies whether this crash occurred on a trafficway that is part\n",
    "of the National Highway System. We can obtain this information from **ROUTE** attribute.*</li>\n",
    "<li> **TWAY_ID**  *-- This data element records the trafficway on which the crash occurred. This information can be derived from **ROUTE** attribute.*</li>\n",
    "<li> **DATETIME** *-- this one because its type datetime had caused us problems in other projects particularly with the PCA analysis, which we will run again. \n",
    "\n",
    "A new dataframe named **df_reduced** will be created to store the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe \"df_reduced\" to store new data from df_accident\n",
    "df_reduced = df_accident\n",
    "# Drop afore mentioned attributes from dataframe df_reduced\n",
    "df_reduced.drop('ST_CASE', axis= 1, inplace =True)\n",
    "df_reduced.drop('COUNTY', axis= 1, inplace =True)\n",
    "df_reduced.drop('SP_JUR', axis= 1, inplace =True)\n",
    "df_reduced.drop('RD_OWNER', axis= 1, inplace =True)\n",
    "df_reduced.drop('RUR_URB', axis= 1, inplace =True)\n",
    "df_reduced.drop('LONGITUD', axis= 1, inplace =True)\n",
    "df_reduced.drop('LATITUDE', axis= 1, inplace =True)\n",
    "df_reduced.drop('STATE', axis= 1, inplace =True)\n",
    "df_reduced.drop('FUNC_SYS', axis= 1, inplace =True)\n",
    "df_reduced.drop('TYP_INT', axis= 1, inplace =True)\n",
    "df_reduced.drop('NHS', axis= 1, inplace =True)\n",
    "df_reduced.drop('TWAY_ID', axis= 1, inplace =True)\n",
    "df_reduced.drop('DATETIME', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>PVH_INVL</th>\n",
       "      <th>PEDS</th>\n",
       "      <th>PERNOTMVIT</th>\n",
       "      <th>PERMVIT</th>\n",
       "      <th>PERSONS</th>\n",
       "      <th>DAY_WEEK</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>ROUTE</th>\n",
       "      <th>HARM_EV</th>\n",
       "      <th>MAN_COLL</th>\n",
       "      <th>RELJCT1</th>\n",
       "      <th>RELJCT2</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "      <th>REL_ROAD</th>\n",
       "      <th>LGT_COND</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>FATALS</th>\n",
       "      <th>DRUNK_DR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30828.000000</td>\n",
       "      <td>30828.000000</td>\n",
       "      <td>30828.000000</td>\n",
       "      <td>30828.000000</td>\n",
       "      <td>30828.000000</td>\n",
       "      <td>30828.000000</td>\n",
       "      <td>30828.000000</td>\n",
       "      <td>30828.000000</td>\n",
       "      <td>30603.000000</td>\n",
       "      <td>29558.000000</td>\n",
       "      <td>30811.000000</td>\n",
       "      <td>30759.000000</td>\n",
       "      <td>30816.000000</td>\n",
       "      <td>30788.000000</td>\n",
       "      <td>30828.000000</td>\n",
       "      <td>30787.000000</td>\n",
       "      <td>30678.000000</td>\n",
       "      <td>30572.000000</td>\n",
       "      <td>30828.000000</td>\n",
       "      <td>30828.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.560335</td>\n",
       "      <td>1.525139</td>\n",
       "      <td>0.035195</td>\n",
       "      <td>0.213605</td>\n",
       "      <td>0.222687</td>\n",
       "      <td>2.290742</td>\n",
       "      <td>2.299825</td>\n",
       "      <td>4.101661</td>\n",
       "      <td>12.809202</td>\n",
       "      <td>3.381724</td>\n",
       "      <td>18.107819</td>\n",
       "      <td>1.589356</td>\n",
       "      <td>0.043257</td>\n",
       "      <td>2.001072</td>\n",
       "      <td>0.034968</td>\n",
       "      <td>2.197031</td>\n",
       "      <td>1.821338</td>\n",
       "      <td>2.742313</td>\n",
       "      <td>1.091475</td>\n",
       "      <td>0.281984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.896586</td>\n",
       "      <td>0.860262</td>\n",
       "      <td>0.264386</td>\n",
       "      <td>0.481211</td>\n",
       "      <td>0.508731</td>\n",
       "      <td>1.957052</td>\n",
       "      <td>1.961892</td>\n",
       "      <td>2.090714</td>\n",
       "      <td>6.868348</td>\n",
       "      <td>1.613226</td>\n",
       "      <td>13.944029</td>\n",
       "      <td>2.506745</td>\n",
       "      <td>0.203438</td>\n",
       "      <td>2.833917</td>\n",
       "      <td>0.298258</td>\n",
       "      <td>1.583026</td>\n",
       "      <td>0.973719</td>\n",
       "      <td>3.397831</td>\n",
       "      <td>0.367088</td>\n",
       "      <td>0.466398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           VE_TOTAL      VE_FORMS      PVH_INVL          PEDS    PERNOTMVIT  \\\n",
       "count  30828.000000  30828.000000  30828.000000  30828.000000  30828.000000   \n",
       "mean       1.560335      1.525139      0.035195      0.213605      0.222687   \n",
       "std        0.896586      0.860262      0.264386      0.481211      0.508731   \n",
       "min        1.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "25%        1.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "50%        1.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "75%        2.000000      2.000000      0.000000      0.000000      0.000000   \n",
       "max       58.000000     58.000000     11.000000     16.000000     16.000000   \n",
       "\n",
       "            PERMVIT       PERSONS      DAY_WEEK          HOUR         ROUTE  \\\n",
       "count  30828.000000  30828.000000  30828.000000  30603.000000  29558.000000   \n",
       "mean       2.290742      2.299825      4.101661     12.809202      3.381724   \n",
       "std        1.957052      1.961892      2.090714      6.868348      1.613226   \n",
       "min        0.000000      0.000000      1.000000      0.000000      1.000000   \n",
       "25%        1.000000      1.000000      2.000000      7.000000      2.000000   \n",
       "50%        2.000000      2.000000      4.000000     14.000000      3.000000   \n",
       "75%        3.000000      3.000000      6.000000     19.000000      4.000000   \n",
       "max       93.000000     93.000000      7.000000     23.000000      7.000000   \n",
       "\n",
       "            HARM_EV      MAN_COLL       RELJCT1       RELJCT2      WRK_ZONE  \\\n",
       "count  30811.000000  30759.000000  30816.000000  30788.000000  30828.000000   \n",
       "mean      18.107819      1.589356      0.043257      2.001072      0.034968   \n",
       "std       13.944029      2.506745      0.203438      2.833917      0.298258   \n",
       "min        1.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "25%        9.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "50%       12.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "75%       30.000000      2.000000      0.000000      2.000000      0.000000   \n",
       "max       73.000000     11.000000      1.000000     20.000000      4.000000   \n",
       "\n",
       "           REL_ROAD      LGT_COND       WEATHER        FATALS      DRUNK_DR  \n",
       "count  30787.000000  30678.000000  30572.000000  30828.000000  30828.000000  \n",
       "mean       2.197031      1.821338      2.742313      1.091475      0.281984  \n",
       "std        1.583026      0.973719      3.397831      0.367088      0.466398  \n",
       "min        1.000000      1.000000      1.000000      1.000000      0.000000  \n",
       "25%        1.000000      1.000000      1.000000      1.000000      0.000000  \n",
       "50%        1.000000      2.000000      1.000000      1.000000      0.000000  \n",
       "75%        4.000000      2.000000      2.000000      1.000000      1.000000  \n",
       "max       11.000000      5.000000     12.000000     10.000000      3.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of rows is       ', 30828)\n",
      "('The number of attributes is ', 20)\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_reduced.shape[0])\n",
    "print ('The number of attributes is ', df_reduced.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30828 entries, 0 to 32165\n",
      "Data columns (total 20 columns):\n",
      "VE_TOTAL      30828 non-null int64\n",
      "VE_FORMS      30828 non-null int64\n",
      "PVH_INVL      30828 non-null int64\n",
      "PEDS          30828 non-null int64\n",
      "PERNOTMVIT    30828 non-null int64\n",
      "PERMVIT       30828 non-null int64\n",
      "PERSONS       30828 non-null int64\n",
      "DAY_WEEK      30828 non-null int64\n",
      "HOUR          30603 non-null float64\n",
      "ROUTE         29558 non-null float64\n",
      "HARM_EV       30811 non-null float64\n",
      "MAN_COLL      30759 non-null float64\n",
      "RELJCT1       30816 non-null float64\n",
      "RELJCT2       30788 non-null float64\n",
      "WRK_ZONE      30828 non-null int64\n",
      "REL_ROAD      30787 non-null float64\n",
      "LGT_COND      30678 non-null float64\n",
      "WEATHER       30572 non-null float64\n",
      "FATALS        30828 non-null int64\n",
      "DRUNK_DR      30828 non-null int64\n",
      "dtypes: float64(9), int64(11)\n",
      "memory usage: 4.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_reduced.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deletion of observations from certain attributes based on a cutoff of 10.\n",
    "\n",
    "We will delete those levels from attribute **HARM_EV**-- which describes the first injury or damage producing event of the crash-- than contain less than 10 events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0    11791\n",
       "8.0      4724\n",
       "1.0      2668\n",
       "42.0     2211\n",
       "33.0     1045\n",
       "34.0      935\n",
       "35.0      884\n",
       "9.0       772\n",
       "24.0      715\n",
       "30.0      650\n",
       "32.0      405\n",
       "59.0      397\n",
       "38.0      376\n",
       "14.0      337\n",
       "31.0      279\n",
       "53.0      257\n",
       "25.0      251\n",
       "43.0      251\n",
       "5.0       204\n",
       "11.0      174\n",
       "52.0      165\n",
       "18.0      144\n",
       "15.0      139\n",
       "23.0       92\n",
       "21.0       88\n",
       "10.0       87\n",
       "39.0       78\n",
       "58.0       76\n",
       "19.0       65\n",
       "17.0       64\n",
       "57.0       60\n",
       "26.0       48\n",
       "3.0        46\n",
       "41.0       45\n",
       "46.0       37\n",
       "20.0       34\n",
       "7.0        32\n",
       "16.0       31\n",
       "54.0       24\n",
       "48.0       23\n",
       "44.0       22\n",
       "40.0       20\n",
       "49.0       17\n",
       "45.0       13\n",
       "51.0       10\n",
       "2.0         9\n",
       "50.0        9\n",
       "73.0        3\n",
       "72.0        3\n",
       "6.0         1\n",
       "Name: HARM_EV, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced.HARM_EV.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the frequencies for each of the 73 levels of the attribute **HARM_EV**, the following levels that do not contain at least 10 observations will be deleted:\n",
    "<ul>\n",
    "<li>-- **2**  *(Fire/Explosion)*</li>\n",
    "<li>-- **6**  *(Injured in Vehicle (Non-Collision))*</li>\n",
    "<li>-- **50** *(Bridge Overhead Structure)*</li>\n",
    "<li>-- **73** *(Object Fell From Motor Vehicle In-Transport)*</li>\n",
    "<li>-- **72** *(Cargo/Equipment Loss or Shift (Harmful to This Vehicle))*</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Delete levels that do not have at least 10 observations\n",
    "df_reduced = df_reduced[df_reduced.HARM_EV != 2]\n",
    "df_reduced = df_reduced[df_reduced.HARM_EV != 50]\n",
    "df_reduced = df_reduced[df_reduced.HARM_EV != 73]\n",
    "df_reduced = df_reduced[df_reduced.HARM_EV != 72]\n",
    "df_reduced = df_reduced[df_reduced.HARM_EV != 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will delete the levels of those categorical variables that have less than 10 observations. \n",
    "<li> **RELJCT2** *-- delete levels 16 (Shared-Use Path Crossing) and 17 (Acceleration/Deceleration Lane)*</li>\n",
    "<li> **WRK_ZONE** *-- delete level 3 (Utility)*</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_reduced = df_reduced[df_reduced.RELJCT2 != 16] # \n",
    "df_reduced = df_reduced[df_reduced.RELJCT2 != 17]\n",
    "df_reduced = df_reduced[df_reduced.WRK_ZONE != 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 18 observations we deleted from those levels of the categorical attributes that did not have at least 10 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of rows is       ', 30782)\n",
      "('The number of attributes is ', 20)\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_reduced.shape[0])\n",
    "print ('The number of attributes is ', df_reduced.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VE_TOTAL      False\n",
       "VE_FORMS      False\n",
       "PVH_INVL      False\n",
       "PEDS          False\n",
       "PERNOTMVIT    False\n",
       "PERMVIT       False\n",
       "PERSONS       False\n",
       "DAY_WEEK      False\n",
       "HOUR           True\n",
       "ROUTE          True\n",
       "HARM_EV        True\n",
       "MAN_COLL       True\n",
       "RELJCT1        True\n",
       "RELJCT2        True\n",
       "WRK_ZONE      False\n",
       "REL_ROAD       True\n",
       "LGT_COND       True\n",
       "WEATHER        True\n",
       "FATALS        False\n",
       "DRUNK_DR      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values, we expect those to be in the categorical variables but not the continuous ones\n",
    "df_reduced.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the continuous variables have missing values:\n",
    "\n",
    "<li> **VE_TOTAL**</li>\n",
    "<li> ** VE_FORMS**</li>\n",
    "<li> ** PVH_INVL**</li>\n",
    "<li> **PEDS**</li>\n",
    "<li> **PERNOTMVIT**</li>\n",
    "<li> **PERMVIT**</li>\n",
    "<li> **PERSONS**</li>\n",
    "<li> **FATALS**</li>\n",
    "<li> **DR_DRIVER**</li>\n",
    "\n",
    "\n",
    "Imputation cannot be performed on categorical attributes, and <a href=\"https://pandas.pydata.org/pandas-docs/stable/missing_data.html\">Pandas</a> treats missing values as zero, so deleting missing values from dataframe is the best option. \n",
    "\n",
    "So in the following two cells, we will get rid of missing values from the whole data set, and then check that the missing values are in fact out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values from data set\n",
    "df_reduced.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VE_TOTAL      False\n",
       "VE_FORMS      False\n",
       "PVH_INVL      False\n",
       "PEDS          False\n",
       "PERNOTMVIT    False\n",
       "PERMVIT       False\n",
       "PERSONS       False\n",
       "DAY_WEEK      False\n",
       "HOUR          False\n",
       "ROUTE         False\n",
       "HARM_EV       False\n",
       "MAN_COLL      False\n",
       "RELJCT1       False\n",
       "RELJCT2       False\n",
       "WRK_ZONE      False\n",
       "REL_ROAD      False\n",
       "LGT_COND      False\n",
       "WEATHER       False\n",
       "FATALS        False\n",
       "DRUNK_DR      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that missing values are in fact gone\n",
    "df_reduced.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of missing values eliminated was 1857, which accounts for about 6% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of rows is       ', 28953)\n",
      "('The number of attributes is ', 20)\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_reduced.shape[0])\n",
    "print ('The number of attributes is ', df_reduced.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining attribute's type\n",
    "\n",
    "The categorical attributes from the original data set were recorded as integers not floats, although in cell #7 we can see that many of the categoricals were read as floats for an unknown reason. For the sake of consistency, we will coerce floats to integers. This was not performed previously because presence of missing values prevented us from doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coerce floats to int\n",
    "df_reduced['HOUR'] = df_reduced.HOUR.astype(int)\n",
    "df_reduced['ROUTE'] = df_reduced.ROUTE.astype(int)\n",
    "df_reduced['HARM_EV'] = df_reduced.HARM_EV.astype(int)\n",
    "df_reduced['MAN_COLL'] = df_reduced.MAN_COLL.astype(int)\n",
    "df_reduced['RELJCT1'] = df_reduced.RELJCT1.astype(int)\n",
    "df_reduced['RELJCT2'] = df_reduced.RELJCT2.astype(int)\n",
    "df_reduced['REL_ROAD'] = df_reduced.REL_ROAD.astype(int)\n",
    "df_reduced['LGT_COND'] = df_reduced.LGT_COND.astype(int)\n",
    "df_reduced['WEATHER'] = df_reduced.WEATHER.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28953 entries, 0 to 32165\n",
      "Data columns (total 20 columns):\n",
      "VE_TOTAL      28953 non-null int64\n",
      "VE_FORMS      28953 non-null int64\n",
      "PVH_INVL      28953 non-null int64\n",
      "PEDS          28953 non-null int64\n",
      "PERNOTMVIT    28953 non-null int64\n",
      "PERMVIT       28953 non-null int64\n",
      "PERSONS       28953 non-null int64\n",
      "DAY_WEEK      28953 non-null int64\n",
      "HOUR          28953 non-null int32\n",
      "ROUTE         28953 non-null int32\n",
      "HARM_EV       28953 non-null int32\n",
      "MAN_COLL      28953 non-null int32\n",
      "RELJCT1       28953 non-null int32\n",
      "RELJCT2       28953 non-null int32\n",
      "WRK_ZONE      28953 non-null int64\n",
      "REL_ROAD      28953 non-null int32\n",
      "LGT_COND      28953 non-null int32\n",
      "WEATHER       28953 non-null int32\n",
      "FATALS        28953 non-null int64\n",
      "DRUNK_DR      28953 non-null int64\n",
      "dtypes: int32(9), int64(11)\n",
      "memory usage: 3.6 MB\n"
     ]
    }
   ],
   "source": [
    "df_reduced.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the coercion, we can see that all of the attributes now are of type int as opposed to float. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properly Indexing dataframe\n",
    "\n",
    "For an unknown reason, the index of the whole data set was set to start at 0 as opposed to 1, so it was necessary for our own understanding to switch it back to starting at 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change index to start at 1 not 0\n",
    "df_reduced.index = np.arange(1, len(df_reduced)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical variables\n",
    "\n",
    "We will use one-hot encoding to turn categorical variables with several levels to dummies. This will not affect those that are already binary such as **RELJCT1**, which values are 0 and 1, but we expect a dramatic increase in the number of attributes considering that some of those categoricals have more than 20 levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from ROUTE attribute\n",
    "route_df = pd.get_dummies(df_reduced.ROUTE,prefix='ROUTE')\n",
    "df_reducedt = pd.concat((df_reduced, route_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from HARM_EV attribute\n",
    "harm_df = pd.get_dummies(df_reduced.HARM_EV,prefix='HARM_EV')\n",
    "df_reduced = pd.concat((df_reduced, harm_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from RELJCT2 attribute\n",
    "reljct_df = pd.get_dummies(df_reduced.RELJCT2,prefix='RELJCT2')\n",
    "df_reduced = pd.concat((df_reduced, reljct_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from WRK_ZONE attribute\n",
    "wrk_df = pd.get_dummies(df_reduced.WRK_ZONE,prefix='WRK_ZONE')\n",
    "df_reduced = pd.concat((df_reduced, wrk_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from REL_ROAD attribute\n",
    "relrd_df = pd.get_dummies(df_reduced.REL_ROAD,prefix='REL_ROAD')\n",
    "df_reduced = pd.concat((df_reduced, relrd_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from LGT_COND attribute\n",
    "lgt_df = pd.get_dummies(df_reduced.LGT_COND,prefix='LGT_COND')\n",
    "df_reduced = pd.concat((df_reduced, lgt_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from WEATHER attribute\n",
    "wtr_df = pd.get_dummies(df_reduced.WEATHER,prefix='WEATHER')\n",
    "df_reduced = pd.concat((df_reduced, wtr_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variable from DAY_WEEK attribute\n",
    "wk_df = pd.get_dummies(df_reduced.DAY_WEEK,prefix='DAY_WEEK')\n",
    "df_reduced = pd.concat((df_reduced, wk_df), axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the one-hot encoding of the above attributes, we delete those variables from which they were derived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To clean df_accident, drop those attributes from which the dummy variables were derived\n",
    "    \n",
    "if 'ROUTE' in df_reduced:\n",
    "    del df_reduced['ROUTE'] \n",
    "\n",
    "    \n",
    "if 'HARM_EV' in df_reduced:\n",
    "    del df_reduced['HARM_EV'] \n",
    "    \n",
    "if 'RELJCT2' in df_reduced:\n",
    "    del df_reduced['RELJCT2'] \n",
    "\n",
    "    \n",
    "if 'WRK_ZONE' in df_reduced:\n",
    "    del df_reduced['WRK_ZONE'] \n",
    "    \n",
    "if 'REL_ROAD' in df_reduced:\n",
    "    del df_reduced['REL_ROAD'] \n",
    "    \n",
    "if 'LGT_COND' in df_reduced:\n",
    "    del df_reduced['LGT_COND'] \n",
    "    \n",
    "if 'WEATHER' in df_reduced:\n",
    "    del df_reduced['WEATHER'] \n",
    "    \n",
    "\n",
    "if 'DAY_WEEK' in df_reduced:\n",
    "    del df_reduced['DAY_WEEK']\n",
    "    \n",
    "df_passenger = df_reduced.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the number of attributes rose from 21 to 104. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of rows is       ', 28953)\n",
      "('The number of attributes is ', 104)\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_reduced.shape[0])\n",
    "print ('The number of attributes is ', df_reduced.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of new dataframe for Task 1\n",
    "\n",
    "Considering that according to a 2009 report from <a href=\"https://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving_slide_2.html?thisspeed=25000\"> AAA published by Forbes</a>: \"According to the IIHS, an average 6.6 people are killed between the hours of 5 p.m. and 6 p.m., and another 6.6 between the hours of 6 p.m. and 7 p.m. Those rates are the overall highest of any time during the day. In 2007, 14,055 people were killed in the 5 p.m. hour. But the hours between midnight and 4 a.m. have the highest number of fatalities when calculated as a percentage of the amount of people on the road, according to AAA. During that time, statistically speaking, 5.87 per 100 million people on the road will be killed.\"\n",
    "\n",
    "Another report, from 2015, published by <a href=\"https://www.forbes.com/2009/01/21/car-accident-times-forbeslife-cx_he_0121driving_slide_2.html?thisspeed=25000\"> Refinery29 based on data from the National Highway Traffic Safety Administration (NHTSA)</a>, states that crashes were fatalities are present tend to occur on the weekends between midnight and 3 a.m.\n",
    "\n",
    "Based on this information, we will create a new dataframe called **df_task1** in which we will store the transformation of our attribute **HOUR** from continuous (from 0 to 23) to ordinal in the following way:\n",
    "\n",
    "<li> **1** = 0-3 a.m. (when supposedly most of the crashes involving fatalities occur)</li>\n",
    "<li> **2** = 4-7 a.m. (morning hours before hitting the road for most people)</li>\n",
    "<li> **3** = 8-11 a.m. (rush hour to get to work)</li>\n",
    "<li> **4** = 12-15 p.m. (lunch hour)</li>\n",
    "<li> **5** = 16 - 19 p.m. (peak hour after work) </li>\n",
    "<li> **6** = 20-23 (night time)</li>\n",
    "\n",
    "Tried using pd.cut but could not get it to work the bins as we wanted, so we tried the harder way by creating a new column **HR_RANGE** derived from **HOUR**, which we will drop next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task1 = df_reduced[df_reduced.columns[:104]]\n",
    "\n",
    "df_task1['HR_RANGE'] = 0\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] <=3] = 1\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] ==4] = 2\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] ==5] = 2\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 6] = 2\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 7] = 2\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 8] = 3\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 9] = 3\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 10] = 3\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 11] = 3\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 12] = 4\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 13] = 4\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 14] = 4\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 15] = 4\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 16] = 5\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 17] = 5\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 18] = 5\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 19] = 5\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 20] = 6\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 21] = 6\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 22] = 6\n",
    "df_task1['HR_RANGE'][df_task1['HOUR'] == 23] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task1['HR_RANGE'] = df_task1.HR_RANGE.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPgBkgsAHgEwQ+ACQif8DECC97GfOF4IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9c7f400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_reduced.plot(x='HR_RANGE', y = 'FATALS', kind = 'scatter')\n",
    "\n",
    "df_HR= df_task1.groupby(by=['HR_RANGE'])\n",
    "\n",
    "# Sum number of Fatalities\n",
    "fatal_Week = df_HR.FATALS.sum()\n",
    "\n",
    "# Graph Chart\n",
    "plt.xlabel('Total Fatalities')\n",
    "g_fatal_Week = fatal_Week.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can see that the highest total number of fatalities are located on level 5 (between 16 and 19 hrs), followed by level 6 (from 20 to 23 hrs). We were expected based on the aforementioned reports that the level of the attribute **HR_RANGE** that will display the higher number of fatalities would be 1 (from 0 to 3 hrs). Nevertheless, our plot does support the findings of the AAA report in the sense that \"peak hours\" (from 16 to 19 hrs) are also those that overall represent the highest incidences of fatalities occurring during day time hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task1.drop('HOUR', axis= 1, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, the continuous attribute **FATALS** will be derived in a new binary variable called **FATALITIES**--our response variable for **Task 1**--, which classes will represent: 0 = single fatality; 1 = multiple fatalities. \n",
    "The attribute **FATALS** will be dropped from the dataframe **df_task1**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26795\n",
       "1     2158\n",
       "Name: FATALITIES, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since our chosen response variable FATALS is continuous with values ranging from 1-99, transform it into a new binary attribute 'FATALITIES'\n",
    "# Where 0 = single fatality and 1 = multiple fatalities. \n",
    "df_task1['FATALITIES'] = 0\n",
    "df_task1['FATALITIES'][df_task1['FATALS'] == 1] = 0\n",
    "df_task1['FATALITIES'][df_task1['FATALS'] > 1] = 1\n",
    "df_task1['FATALITIES'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite obvious that the classes for our response variable are imbalanced with 7% corresponding to class 1 and 93% to class 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the original response variable from the dataframe so it won't influence results. \n",
    "if 'FATALS' in df_task1:\n",
    "    del df_task1['FATALS'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will separate our explanatory variables, which we will call **X**, and our response variable **FATALITIES**, which now will be called **y** from our df_reduced dataframe. This way when performing a dimensionality reduction using PCA, considering that our dataframe has 104 columns, the response variable will not be included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'FATALITIES' in df_task1:\n",
    "    y = df_task1['FATALITIES'].values\n",
    "    del df_task1['FATALITIES']\n",
    "    X = df_task1.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction for Task 1 data: PCA \n",
    "\n",
    "As mentioned prior, we will perform PCA on the explanatory variables (104 columns), now known as **X**, of our dataframe to reduce its dimensionality. To decide the number of components that will fit our data better, we will start by choosing 10, and from there we will decide if we should increment or reduce the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.54336244e+00,   3.61399573e-01,  -4.43270847e-04, ...,\n",
       "          3.27238321e-03,  -8.90199840e-04,   4.78172220e-02],\n",
       "       [  3.61399573e-01,   2.54432275e+00,  -5.17038288e-04, ...,\n",
       "          3.32873152e-03,  -1.01180758e-03,   5.23793181e-02],\n",
       "       [ -4.43270847e-04,  -5.17038288e-04,   2.18247991e+00, ...,\n",
       "         -5.63483088e-05,   1.21607743e-04,  -4.56209609e-03],\n",
       "       ..., \n",
       "       [  3.27238321e-03,   3.32873152e-03,  -5.63483088e-05, ...,\n",
       "          2.18249491e+00,  -7.58499425e-05,   6.75426103e-03],\n",
       "       [ -8.90199840e-04,  -1.01180758e-03,   1.21607743e-04, ...,\n",
       "         -7.58499425e-05,   2.18262220e+00,  -4.90691058e-03],\n",
       "       [  4.78172220e-02,   5.23793181e-02,  -4.56209609e-03, ...,\n",
       "          6.75426103e-03,  -4.90691058e-03,   2.90921504e+00]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEeCAYAAACOtbLLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8XHW9//HXJ2nTNGnatE3btE3btFC6sXQJOwjIJiCC\nFBAXBEVxX64LgoqgXAW5rvf+FC+iLBdQKVQ2ERBEkcVqutGFlkK3JG2adE+TZv/8/jgnZRLSZtLO\n5Ewy7+fjMY85c+Ysn0yS85nv93u+36+5OyIiIm0yog5ARERSixKDiIi0o8QgIiLtKDGIiEg7Sgwi\nItKOEoOIiLSjxCDSi5jZPWame8wlqZQYJOWZ2SQzu9PMVplZnZntMLPXzexeMzsj6vgOlZndbGYe\n82g1s+1m9hcze2+Cz3Wxmd2cyGNK39Mv6gBEDsTMSoC/A03AfcAKYCAwGTgHqAFeiCzAxPoOsI7g\n//II4FPAE2b2YXd/MEHnuBi4Crg5QceTPkiJQVLdTUAOMNPdl3Z808wKE3UiM8tz95pEHe8g/Nnd\nS2PieQQoBb4FJCoxiHRJVUmS6iYD2zpLCgDuXtlxnZmdYWZ/MrNtZlZvZmvN7DdmVhC+XxxW2dxs\nZh8ws4Vmthf4n5hjjDazO8xso5k1mtmmsDprZCfnG2JmPzSzN82swcyqzex3ZjbpUH5wd18IbAMO\n72pbMzvazP4Y8zOvNLPrzCwzZpu/EZQW6FB1dfWhxCl9j0oMkureAqaY2SXuPr+rjc3sU8AdQEX4\nvAEYD1wIFAFbYza/GPhiuN2vgN3hMcYDrwJZwG/CGA4HPgOcYWYl7r4r3HYI8Ep4jt8SVHWNBj4L\nLAi33XAwP3iYyIYC70h+HbaLrW77Rbj9hcAPgWOAD4ebfp/gy+CpwJUxh3jlYOKTPszd9dAjZR/A\niUAj4MAbBBffzwDTOtm2CGgAVgL5nbyfET4Xh8dr2s9xHgOqgKIO60uAZuDmmHU/B/YCx3TYdgJB\norknjp/x5jCeM4ECoBB4F/BSuP7WmG3vCf5t2+3/chjX0THrDHio7bgH2l8PPTo+VJUkKc3dXwXm\nAPcCQ4CPAb8EVprZix2qay4j+Jb/XXff2cmxWjus+pO7vx67IiwBvBd4HKg3s4K2B7AeeJOg0Rsz\nM4Jv4y8CFR22rQX+2bZtnJ4DqoHNBCWAWcBPgBv3t0NYtXUS8Li7vxbzszpBCQHg/d2IQURVSZL6\n3H0ZcDWAmU0ATgM+QVAl8piZzXH3RoL2CIDFcR76jU7WTSGobrkmfHRmbfg8AhhOcPGv3s+2HZPR\ngXwujKkV2Am87u57u9hnYvi8opP3Xg+PdUhtHZJ+lBikV/Ggvv4+M/s/4B/AycBxBNUu3VXXyToL\nn+8nKKV0Zm+HbZ8jqM8/VP/ymLuSRKKixCC9kru7mS0gSAxjw9VtJYCZdF4aiMebBPXyWe7+XBfb\nVhN8sx8cx7bJsi58ntHJe1MJSj9rY9ap17R0SW0MktLM7Gwze8cXGDMbyNv19yvD54cJGqpvMrPB\nnexjHdd15O7bgKeAS8zshM6OYWYjwm1bgQeA48zs0v3E/47bWxPJ3asI7iq60MyOjI0TuCF8+ceY\nXfaE7w9LZlzSu6nEIKnup8BwM3scWEZQ/TMO+BBB7+D7wjYI3L3czL5McMvmMjO7j+B21bHARcDH\ngSVxnPMzBFVTL4bHWEzwJWpSeJz7eLvn8LcISi0PmdlDBA3OjQR3JZ0PLCRsH0miLxE0Vv/DzNpu\nV30vcC7woLs/H7PtP4HPA780sz8R3Jm1wN3XIRJSYpBU9xWCi/EpwFwgH9gFvEZQr39P7MbufoeZ\nvQV8naCPwgBgE/A8UBbPCd29zMzmAN8Iz/0RoD7c/wmC20Dbtt1lZicDXwUuD7dvBsoJkstdB/Ez\nd4u7l5rZScB3CfpP5BJUH30D+HGHzX9HcLfTFQR3cWUQ3OmlxCD7WHBXm4iISEBtDCIi0o4Sg4iI\ntKPEICIi7SgxiIhIO73yrqSCggIvLi6OOgwRkV5l4cKFW919RFfb9crEUFxcTGmpRg4QEekOM4tr\nCHhVJYmISDtKDCIi0o4Sg4iItKPEICIi7SgxiIhIO0oMIiLSjhKDiIi00yv7MYiI9HWtrc622ka2\n7K6nclc9lbvr2bK7nsvmjGP88JyknrtHE4OZfQn4JMFcub9295+FM0n9ASgG1gOXu/uOnoxLRKQn\n1Te1tLvYd1zesruBqpp6mlraT4uQYTB7/NC+kxjCaQc/STBxeyPwtJk9CVwLPO/ut5nZ9cD1BBOM\niIj0Ku7O9trGmIt8Q7Ace+HfXc/OuqZ37JuTlUnh4GwKh2Rz/MRhjBqSTeHgbEaF6woHZ1MwKIt+\nmclvAejJEsM0gikE6wDM7O/AJQQzXp0ebnMv8DeUGEQkxdQ3tVC1O7jQx17sY5erdjfQ2NLabj8z\nKBg0gMLB2RQNzaGkeOg7LvijhmSTN6AfcUxL3iN6MjEsB75vZsOBvQTz4ZYCo9x9c7hNJTCqs53N\n7FqC0gXjx49PfrQiklZ21zdRsWMv5Tv2UrGjjoqd4XL4vL228R37ZPfP2Pctv2TC0H3f8tsu9oWD\nsxmRN4D+PfAtP5F6LDG4++tm9kPgWaCWYFL2lg7buJl1Oteou98J3AlQUlKi+UhFJG7uzs66pvBC\nX0f5jvYX/Yoddeyub263z4B+GYwdOpCx+QOZMWYwo4cMbHfBLxyczeCBqfMtP5F6tPHZ3X8D/AbA\nzH5AMGH6FjMb7e6bzWw0UNWTMYlI7+fuVO9poKLdxX4v5THf/Osa230PJTcrk6KhOYwdOpBji4cy\nNn8gY4cODNblD6RgUFafvOjHo6fvShrp7lVmNp6gfeEEYCJwFXBb+PxYT8YkIqmvpdWpqql/u6pn\nZ3DRb0sAFTv30tDcvm5/yMD+jM0fSPHwXE4+vGDfBb9oaPAYMrB/2l74u9LT/RgeCdsYmoDPuftO\nM7sNeMjMrgE2AJf3cEwikgIamlso217Huq11bNhWy7qttazfVkvZ9r1s3rX3HbduFgzKYmz+QKaN\nHsxZ00ftu+i3Vf/kZfeP6Cfp/Xq6KunUTtZtA87syThEJBqNza2U7ahj/dbgwr9hWx3rwySwaede\nWmOu/fk5/SkensvMcflccPTo4KKf/3ZVz8CszOh+kD5OPZ9FJKGaWlop37F338W/7cK/YVsd5Tvq\n2l38B2f3Y2JBLnMmDGXu7CImFuQyYXgOEwtyyc/Jiu6HSHNKDCLSbc1tF/9ttazfWsv6bXX7kkD5\njr20xFz98wb0o7ggl2PG5XPxzDEUF+QGj+G5DM1RPX8qUmIQkU61tDoVO/ayblvt23X+YRIo215H\nc8zFPzcrk+KCXI4cO4QLjw4u/hMLcigensuw3PS9u6e3UmIQSXOtrU7Fzr2sqqxhdeVuVm/Zw+rK\n3azbWtuuwTcnK5Pi4blMG53HeUcWhhf/4Jt/Ot/a2RcpMYikkR21jTEJoIZVlTW8UVlDbcw9/kVD\nBzK1MI93Tx2171v/xIJcRuQN0MU/TSgxiPRB9U0tvFm1Z18SCJ5rqKpp2LdNfk5/pozK49I5RUwp\nHMyUwjyOGDVIt3mKEoNIb9ba6mzcXrfvwr96S5AE1m+t3Xf3T1a/DCaPHMQpkwuYWpjHlMLBTC3M\nY6RKALIfSgwivcTWPQ2srqx5uyqosoY3tuxhb1NQDWQG44flMGVUHu89avS+UkDx8JweGapZ+g4l\nBpEUU9fYzJote95OAluCJLB1z9ujew7PzWJKYR5XHDduXyngiFGDyMnSv7QcOv0ViUSopdV5s2oP\nizfuYPHGnSwu28Gaqj14WA2U3T+DI0blccaUkUwpzGNqWAoYkTcg2sClT1NiEOlB22sb2yWBpWW7\n2NMQDPecn9OfWePyOe/I0UwbHZQCxg/LITND7QDSs5QYRJKkqaWVVZtrWFwWJoKNO1i/rQ6AzAxj\namEeF88aw+zxQ5k1fijFw3PUGCwpQYlBJEEqd9UHpYGyIAm8Vr5r31DQI/IGMHt8PlccN55Z4/I5\nqmiI2gMkZekvU+Qg1De1sGLTrrAksJNFG3eweVc9AFmZGRw5djAfOWECs8bnM2v8UMYMyVZpQHoN\nJQaRLrg7Zdv3tqsSWrl5977hIoqGDqSkeBizxuUza3w+08cMZkA/DQktvZcSg0gHexqaea18577S\nwJKyHftuFR3YP5Njxg3hE6dOYta4fGaOz2dkXnbEEYsk1n4Tg5ndGe9B3P3axIQj0vNaWp1X3trK\nn5dXsmjDDt7YUrOv1/CkEbmcdsRIZk/IZ9a4oRwxapA6i0mfd6ASw7gOr08Kn1eEzzMAB15JdFAi\nPeHNqhoeXljBo4srqNxdz6AB/Zg9YSjnzihk1vh8Zo7L12Qxkpb2mxjc/by2ZTO7DqgDrnb3mnBd\nHvBbYGGygxRJlB21jTy+dBPzF5WztHwXmRnG6UeM4Mb3TufMaSPJ7q+2AZF42xi+DJzVlhQA3L3G\nzG4GngNuS0JsIgnR2NzKC6urmL+onL+uqqKpxZk+ejDfvmAaF80cq17EIh3EmxjygEJgZYf1hcCg\nhEYkkgDuzrKKXcxfVMHjSzexvbaRgkEDuOrEYubOKWLa6MFRhyiSsuJNDH8E7jazrwL/DNedAPxX\n+J5ISqjcVc+jSyp4ZGE5a6r2kNUvg7Onj+LS2UWcOrlADccicYg3MXwa+BnwQMw+LcA9wFcSH5ZI\n/PY2tvDMikoeWVTOy29updVhzoSh/OD9R3HB0aMZMlATz4h0R1yJwd3rgGvN7GvA4eHqN919d9Ii\nEzmA1lbnX+u3M39ROU8tq2RPQzNj8wfy+TMO55LZRRQX5EYdokiv1d0ObhmAAcvcvbGrjUUSbf3W\nWuYvrmD+onLKd+wlNyuT848azdw5RRxXPIwMjUQqcsjiSgxmNgj4NfABoBU4AlhrZncAm9z9luSF\nKOlu194mnlq2mUcWllO6YQdmcMrhBXztnCmcM2OUBqMTSbB4/6NuA4qB44C/xax/CrglfIgkTHNL\nK/9Ys5VHFpXz7MotNDa3cvjIQXzjPVO5eNYYRg8ZGHWIIn1WvInhImCuu5eamcesXwlMSnxYkq5e\n37yb+YvKeXTJJqprGsjP6c8Hjx3HJbOLOLpoiEYoFekB8SaG4cDWTtYPIhgWQ+SgVdc08PjSTTyy\nsJyVm3fTL8M4Y+pI5s4u4t1TR5LVT7eYivSkeBNDKXAB8D/h67Zk8Eng1UQHJX2fu7O4bCf3vLye\np5ZtprnVObpoCDdfOJ33zRzLsFyNUSQSlXgTwzeBp81serjPl8Llk4HTkhWc9D0NzS08tWwz97y8\nnqXlu8gb0I8rT5zAB48bzxGj8qIOT0SIvx/DS2Z2CvB1YANwPrAIOMndlyYxPukjqnbXc/+CjTy4\nYANb9zQyaUQut1w0g/fPLmLQAN1VJJJK4v6PdPclwIeTGIv0QYs37uCeV9bzp9c20+LOGVNGcvVJ\nxZxyeIH6HIikqG59VTOzkcBIgo5u+7j7a4kMSnq3fdVFr2xgadlO8gb046MnFvPREyeoR7JILxBv\nB7ejgfsJJufp+DXPAQ1iL1TtrueBBRt5YMFGtu5pYNKIXL530QwuUXWRSK8S73/rXcAW4PPAJnSL\nqsRYUraTe15ex5+WbaapxTljygiuPnkip6q6SKRXijcxzABmufsbh3IyM/sP4BMEiWUZ8DEgB/gD\nQc/q9cDl7r7jUM4jydfY3BpWF61nSdlOBg3ox4ePn8BVJxUzUdVFIr1avIlhBUHbwkEnBjMbC3wR\nmO7ue83sIeAKYDrwvLvfZmbXA9cD3zjY80hyVdXU82BYXVRd08Ckgly++74ZzJ2j6iKRviLe/+Rv\nALeb2TcJvuk3xb7ZjeG3+wEDzayJoKSwCbgBOD18/16CsZiUGFLM0rKd3PPKep58bRNNLc7pU0Zw\n9UnFvGvyCFUXifQx8SaG5zs8d9Rl47O7V5jZj4CNwF7gWXd/1sxGufvmcLNKYFRn+5vZtcC1AOPH\nj48zbDkUjc2t/Hl5UF20eOPb1UUfPXECk0ZoRleRvirexHD2oZ7IzIYSDMY3EdgJzDOzj8Ru4+7e\nYZC+2PfuBO4EKCkpUeN3ElXXNPDggo3cv2AD1TUNTCzI5eYLpzN3ThF52ZoNTaSvi7fn8/5KCt1x\nFrDO3asBzGw+cBKwxcxGu/tmMxsNVCXgXHIQlpbt5N5X1vPka5tpbGnltCNGcPWlxZym6iKRtLLf\nxBD2XVju7q3h8n7F2cFtI3CCmeUQVCWdSTA4Xy1wFcGcD1cBj8UZuyRAx+qi3KxMPnT8eK48cQKH\nqbpIJC0dqMSwBCgk+Aa/hOAW09ivjW2v4+rg5u4LzOxhgjGWmoHFBFVDg4CHzOwagnGYLu/+jyEH\n495X1vOLF96kqqaB4uE53HThdC5VdZFI2jtQYpgMVMcsHzJ3vwm4qcPqBoLSg/Sgl9/cyk2Pr+D4\nicP44dyjOe0IVReJSGC/icHd3+psWXq/xuZWbnxsOROG53Dvx48ju79GNBGRtx3MIHrjgXazqLj7\nK4kMSpLr1/9Yy9rqWu7+2LFKCiLyDvEOolcIPMDbHdHa2hba6OrSS5Rtr+N//rqG98wo5IwpI6MO\nR0RSULyT6f4s3PYYgjuKTgc+CKwmmLRHeonvPrGSDDO+c+H0qEMRkRQVb1XS6cCF7r7czFqBSnd/\n0czqCBqTn0lWgJI4z63cwnOvb+GG86YyJn9g1OGISIqKt8SQw9t3KG0HRoTLy4GZiQ5KEm9vYws3\nP7GCySMH8fFTJkYdjoiksHgTw2pgSri8FPhUOFrqZwgGwpMU94sX3qR8x15uufhI+mfG+2sXkXQU\nb1XS/wBjw+VbgKcJ5n9uBK5OfFiSSG9V7+F/X3yLS2aN5YRJw6MOR0RSXLxjJd0Xs1xqZsUE8yis\nd3eNbZTC3J2bHltBdv9Mbjh/WtThiEgvcFAzq7j7HuBfCY5FkuDJ1zbz0ptb+d5FMxiRNyDqcESk\nFzjQIHo/ifcg7v6VxIQjiVRT38QtT67kyLGD+fDxE6IOR0R6iQOVGI6N8xiaGyFF/fQva6je08Cd\nHy0hU+MgiUicDjRW0qk9GYgk1spNu7n31fV86LjxzByXH3U4ItKLdPu+RTMbaGbqHZXCWludGx9b\nzpCB/fn6uVO63kFEJEbcicHMPm9ma4E9wB4zW2dmX0heaHKwHl5YzsINO7jhvKnk52R1vYOISIx4\nB9G7Ffgs8BPg1XD1icAtZjbG3W9IUnzSTTtqG7n1z69zbPFQ5s4uijocEemF4r1d9Vrgk+7+UMy6\nZ83sdeAOQIkhRdz+zGp21zdzy8VHauIdETko8VYlZRBM79nREjTkdspYtHEHv//3Rj52UjFTCwdH\nHY6I9FLxJob7gU93sv5agnkaJGLNLa3c+OhyRuYN4MtnHxF1OCLSi3Wn5/M1ZnYO8M/w9fEEs7nd\nF9sZTp3donH/PzewYtNufvGh2QwacFAd2kVEgPgTw0zgtXC57f7HneEjdthtdXaLQFVNPT9+9g1O\nnVzA+UcVRh2OiPRy8Q6ip85uKewHf3qdhuZWvnfRkZipwVlEDk1cbQxmNv4A7x2XuHCku155ayuP\nLtnEp0+bxMSC3KjDEZE+IN7G56VmdkXsCgt8B3gx8WFJPBqbgwbnccMG8tkzDo86HBHpI+JNDN8G\nfmtm95lZXjgfw0sEdypdlKTYpAu/eWkdb1XX8r33HUl2f901LCKJEVdicPdfEIy2ejSwjKD/wlbg\naHd/Jnnhyf6U76jjv59fwznTR3HG1JFRhyMifUh3BtErA94AxgC5wGPuvjUpUUmXvvfESgC+c+H0\niCMRkb4m3sbnkwluV50EHEXQse1nZjbPzIYmMT7pxF9XbeHZlVv44pmTKRqaE3U4ItLHxFtieAH4\nPXCiu69297uBWcA4gqol6SH1TS3c9PgKDh85iGtOmRh1OCLSB8Xbwe097v7X2BXu/lZYkrgx8WHJ\n/vzyhTcp276XBz95PFn9uj2dhohIl+Lt4PbX/axvAW5OZECyf2ur9/Crv6/l4pljOOmwgqjDEZE+\n6oBfOc3sRTPLj3l9S2ybgpkVhJP3SJK5Ozc9voIB/TL45gXTog5HRPqwruoiTgFipwD7EhDb2JwJ\nTEh0UPJOTy2r5B9rtvLVc45gZF521OGISB/W3UpqDcQTgT0NzXzvyRXMGDOYj5ygPCwiyaXxmXuB\nn/3lDapqGvjVR+bQL1MNziKSXPFcZToOpX1QQ2ub2RQzWxLz2G1mXzazYWb2FzNbEz6rX0SMVZW7\nufuV9Vxx7HhmjddHIyLJF0+J4R4zawiXs4FfmVlt+HpAvCdy99WEczeYWSZQAfwRuB543t1vM7Pr\nw9ffiPe4fVlrq/PtPy5ncHY/rjt3Stc7iIgkQFeJ4UHalxB+3+H92nCb7joTeMvdN5jZRcDp4fp7\ngb+hxADAI4vKKd2wg9vnHs3Q3KyudxARSYADJgZ3/0iSznsF8LtweZS7bw6XK4FRSTpnr7KzrpFb\n/7yKOROGcumcoqjDEZE00uMtmWaWBbwPmNfxPXd39tOGYWbXmlmpmZVWV1cnOcro3f7ManbtbeI/\nLz6SjAzdDCYiPSeKW1zOAxa5+5bw9RYzGw0QPld1tpO73+nuJe5eMmLEiB4KNRpLynbyu39t5KoT\ni5k2enDU4YhImokiMXyQt6uRAB4HrgqXrwIe6/GIUkhLq/PtR5cxYtAA/uPsyVGHIyJpqEcTg5nl\nAmcD82NW3wacbWZrgLPC12nrgQUbWF6xmxvfO5287P5RhyMiaahHO7i5ey0wvMO6bQR3KaW96poG\n/uuZ1ZxyeAHvPXp01OGISJqKu8RgZtPN7Gdm9oSZFYbr3mdmxyQvvPRy61OvU9/UwncvmoGZGpxF\nJBrxzuB2JrAQOAw4B2ibNmwKGnY7If65dhvzF1fwqXcdxmEjBkUdjoiksXhLDN8HrnP3C4HGmPUv\nAMclPKo009jcyo2PLqdo6EA+d8bhUYcjImku3jaGI4EnOlm/lQ5tBtJ9v315HWuq9nDXR0sYmJUZ\ndTgikubiLTHsBMZ0sn42UJ64cNLPpp17+flzazhr2ijOmq5O3yISvXgTw++A28NGZwcywvme/wu4\nP1nBpYPvPbESx7npwulRhyIiAsSfGL5FMBpqBTAIWAm8CPwL+M/khNb3vbCqiqdXVPKFd09m3LCc\nrncQEekBcbUxuHsj8AEzmwzMIUgoi9x9VTKD68vqm1q46fEVTBqRyydPnRR1OCIi+8SVGMysH2Du\nvgZYE7O+P8HYd81Jiq/PuuNvb7Fxex0PfOJ4svppVjYRSR3xXpEeBr7QyfovAA8lLpz0sG5rLXf8\n/S3ed8wYTj68IOpwRETaiTcxnAI83cn6Z8P3pBtufep1sjIz+PYF06IORUTkHeJNDLlAayfrm4G8\nxIXT923ZXc9zr2/hoydOYOTg7KjDERF5h3gTw2vABzpZ/0FgReLC6fseWVROq8NlJeOiDkVEpFPx\n9nz+T2C+mU0C/hquO5MgMVyajMD6Infn4dJyji0eysSC3KjDERHpVFwlBnd/Ang/waB5d4aPI4BL\n3D2tJ9bpjkUbd7B2ay2XzVFpQURSV9zzMbj7k8CTSYylz5tXWs7A/pmcr7kWRCSFdXuiHjMbRIeS\nhrvvTlhEfVRdYzNPvraZC44ezaABPTo/kohIt8Q7H8O4cIKeWmAXsCN87AyfpQtPL69kT0Mzl80p\nijoUEZEDiver6z0Ew2t/BthEMJCedMO80nImDM/huInDog5FROSA4k0MxwMnuvuyZAbTV23cVser\na7fx1bOP0JSdIpLy4u3HsAHon8xA+rKHF5VjBnNVjSQivUC8ieFLwK1mVpy8UPqm1lbnkYXlnHJ4\nAWPyB0YdjohIl+KtSpoH5ABvmVkd0BT7prur4nw/Xl27jYqde7nuPVOiDkVEJC7xJoavJTWKPmxe\naRmDs/tx7ozCqEMREYlLvBP1/CbZgfRFu+ub+PPySi4rKSK7f2bU4YiIxOVgOrgVAFmx69x9U8Ii\n6kOeXLqZhuZWDYEhIr1KvDO4DQZ+SjDCamctqPo63ImHSss4YtQgji4aEnUoIiJxi/eupNuBYwkS\nQz1wJXADUAF8KDmh9W5vVtWwpGwnl80Zp74LItKrxFuVdAHwYXd/0cxagH+5+4NmVgF8HPhD0iLs\npeaVlpOZYVw8a2zUoYiIdEu8JYahBJ3cAHYDbbenvoym9nyH5pZW5i+u4N1TRzIib0DU4YiIdEu8\niWEtMCFcXgVcHi5fBGxPdFC93d/fqKa6pkED5olIrxRvYrgPmB0u3wZ8zszqgZ8AP0pGYL3ZvNJy\nCgZlccbUkVGHIiLSbfH2Y/hRzPJzZjadoDF6jbsvTlZwvdG2PQ08v2oLV51YTP/MePOuiEjqOKgZ\nY9x9HbAuwbH0CY8u2URTi3NZifouiEjvtN/EYGZfBO509/pweb/c/b8THlkv5O7MKy3j6KIhTCnM\nizocEZGDcqASw9eBBwj6LXz9ANs5oMQArNi0m1WVNdxy0YyoQxEROWj7TQzuPq6z5UNhZvnAXcCR\nBAnl48Bqgn4QxcB64HJ375XThc4rLSOrXwbvO0Z9F0Sk9+qyddTM+pvZy2aWiHGjfw487e5TgWOA\n14HrgefdfTLwfPi612lobuGxpZs4d0YhQ3I0p5GI9F5dJgZ3bwImA62HciIzGwK8C/hNeNxGd99J\n0Bfi3nCze4GLD+U8UXluZRU765rUd0FEer1476f8P+CaQzzXRKAauNvMFpvZXWaWC4xy983hNpXA\nqM52NrNrzazUzEqrq6sPMZTEm7ewjNFDsjn58IKoQxEROSTx3q6aBXzCzM4CFgK1sW+6+1fiPNds\n4AvuvsDMfk6HaiN3dzPzznZ29zuBOwFKSko63SYqlbvqefGNaj57+uFkZmjAPBHp3eJNDDOB18Ll\n6R3ei/ciXQ6Uu/uC8PXDBIlhi5mNdvfNZjYaqIrzeCnjkUXltDpcqmokEekD4u35fOqhnsjdK82s\nzMymuPvKxBGcAAAREElEQVRq4ExgZfi4imCojauAxw71XD3J3Xl4YTnHFQ+juCA36nBERA7ZQfV8\nPgRfAB4wsyyCgfk+RtDO8ZCZXUMwguvlB9g/5SzcsIN1W2v57OmHRR2KiEhCxJ0YzOxU4IPAeN45\ntec58RzD3ZcAJZ28dWa8caSaeaXl5GRlcv5Ro6MORUQkIeK6K8nMrgSeA0YAZxPMyTAaOI7gm39a\nqmts5snXNnHBUaPJHdDThS8RkeSI93bV6wjuJroMaASuc/ejgN+RxvMxPLWsktrGFg2YJyJ9SryJ\nYRLwbLjcAAwKl/+bYFiLtDSvtIzi4TkcWzw06lBERBIm3sSwHWgbLrQCaBslLh8YmOigeoON2+pY\nsG47l84pwkx9F0Sk74i3Yvwl4CxgGUH/g5+b2bsJ2hueS1JsKe3hhWWYwSWz1XdBRPqWeBPDF3i7\nZPADgnGTTgYeBb6bhLhSWmur88iiCk6dPIIx+WlZYBKRPuyAicHM8ty9xt23tq1z9xbg+0mPLIW9\n8tY2Knbu5frzpkYdiohIwnXVxlBpZneb2ck9Ek0vMW9hGYOz+3H29E7H+xMR6dW6SgxfI5hU5x9m\ntsrMvmZmI3sgrpS1a28TTy+v5KKZY8nunxl1OCIiCXfAxODud7j7scAsgttVrwfKzGy+mZ1naXg7\nzhNLN9HQ3MplJWp0FpG+Ka7bVd19qbt/ERgDfJSgH8MTwEYz+14S40s58xaWM2VUHkeNHRJ1KCIi\nSRFvPwZg36xrfwjHRroIyAG+lZTIUtCaLTUsLdvJZSXquyAifVe3EoOZ5YUzqf0TeBzYTNAOkRbm\nLSynX4Zx8ayxUYciIpI0cfVjMLPTCIa+mEvQh+Eh4Mvu/s8kxpZSmlpamb+ogndPHUnBoAFRhyMi\nkjRd9WP4FnA1cBiwAPgS8Ht3rz3Qfn3R31dXs3VPgwbME5E+r6sSw5eB/wPucveVPRBPynqotIyC\nQVmcPmVE1KGIiCRVV4lhjLs39UgkKWzrngb+uqqKj51cTP/MbjXLiIj0Ol31Y0j7pADw6OIKmltd\n1Ugikhb09bcL7s7DC8s5pmgIR4zK63oHEZFeTomhC8srdrOqskalBRFJG0oMXZi3sIwB/TK48Jgx\nUYciItIjuj2DvZnNAE4HMoGX3H1RooNKFfVNLTy2ZBPnzihkyMD+UYcjItIjutvz+VPAC8BpwLuB\nv5nZdckILBX8ZeUWdu1t0oB5IpJWuurgNsLdq2NWfRE42t0rw/dPBR4Bbk9eiNGZt7CcMUOyOemw\ngqhDERHpMV2VGP5lZlfHvK4DYqctmw7sTnRQqWDzrr38Y001c+cUkZmhAfNEJH101cZwCvD/zOxK\n4JMEJYZ5ZtY/3LcZuDK5IUZj/qIK3OHSOapGEpH0csDE4O4VwPvNbC7wF+DXwBEEYydlAKvdvT7p\nUfYwd2deaRnHTxzGhOG5UYcjItKj4p2o5xGCWdyKgZeB7HDynj6XFABKN+xg/bY69V0QkbTU5e2q\nZnY+MA1Y6u6fNrNTgN+a2fPAt/riSKvzSsvIzcrk/KMKow5FRKTHHbDEYGY/Bu4GjgX+18xudPeX\ngDnALmBxmDj6jNqGZp58bTMXHD2anKxud/MQEen1uqpKuho4392vIEgOV8K+KT5vAi4GbkhqhD3s\nqWWbqWtsUTWSiKStrhJDLTAxXB4HtGtTcPeV7n5qMgKLyryF5UwsyKVkwtCoQxERiURXieEG4D4z\n2wT8Hbgx+SFFZ8O2Wv61bjuXzinCTH0XRCQ9dXW76gNm9jQwCVjj7jt7JqxoPLywnAyDubPVd0FE\n0leXravuvg3Y1gOxRKql1XlkYTmnTh5B4ZDsqMMREYmMht0OvfLWVjbtqteAeSKS9nr0fkwzWw/U\nAC1As7uXmNkw4A8EnefWA5e7+46ejAvgodJyhgzsz1nTRvX0qUVEUkoUJYYz3H2mu5eEr68Hnnf3\nycDz4esetauuiWdWVHLRzDFk98/s6dOLiKSUVKhKugi4N1y+l6BvRI96/LVNNDa3ctkc9V0QEenp\nxODAc2a20MyuDdeNcvfN4XIl0Gldjplda2alZlZaXV3d2SYH7eHSMqYW5nHk2MEJPa6ISG/U04nh\nFHefCZwHfM7M3hX7prs7QfJ4B3e/091L3L1kxIgRCQvojS01LC3fxWUl49R3QUSEHk4M4TDeuHsV\n8EfgOGCLmY0GCJ+rejKmeaVl9MswLp45pidPKyKSsnosMZhZrpnltS0D5wDLgceBq8LNrgIe66mY\nmlpa+ePiCs6cNpLhgwb01GlFRFJaT96uOgr4Y1hd0w940N2fNrN/Aw+Z2TXABuDyngrohVVVbN3T\nqEZnEZEYPZYY3H0tcEwn67cBZ/ZUHLHmLSynYNAATp+SuDYLEZHeLhVuV43E1j0NvLCqiktmj6Vf\nZtp+DCIi75C2V8RHF1fQ3OpcNkdDYIiIxErLxODuzCstZ+a4fCaPyos6HBGRlJKWiWFZxS5Wb6nR\ngHkiIp1Iy8Qwr7ScAf0yuPAY9V0QEeko7RJDfVMLjy2p4D1HFjI4u3/U4YiIpJy0SwzPrtzC7vpm\n9V0QEdmPtEsM80rLGJs/kJMOGx51KCIiKSmtEsOmnXt56c2tzJ1TREaGBswTEelMWiWG+YvKcUd9\nF0REDiCtEsPIwdl8oGQc44blRB2KiEjK6tE5n6N2eck4Li9Ro7OIyIGkVYlBRES6psQgIiLtKDGI\niEg7SgwiItKOEoOIiLSjxCAiIu0oMYiISDtKDCIi0o65e9QxdJuZVQMbDnL3AmBrAsPp7fR5vE2f\nRXv6PNrrC5/HBHcf0dVGvTIxHAozK3X3kqjjSBX6PN6mz6I9fR7tpdPnoaokERFpR4lBRETaScfE\ncGfUAaQYfR5v02fRnj6P9tLm80i7NgYRETmwdCwxiIjIASgxiIhIO2mVGMzsPWa22szeNLPro44n\nKmY2zsxeMLOVZrbCzL4UdUypwMwyzWyxmT0ZdSxRM7N8M3vYzFaZ2etmdmLUMUXFzP4j/D9Zbma/\nM7PsqGNKtrRJDGaWCfwCOA+YDnzQzKZHG1VkmoGvuvt04ATgc2n8WcT6EvB61EGkiJ8DT7v7VOAY\n0vRzMbOxwBeBEnc/EsgErog2quRLm8QAHAe86e5r3b0R+D1wUcQxRcLdN7v7onC5huCffmy0UUXL\nzIqAC4C7oo4lamY2BHgX8BsAd290953RRhWpfsBAM+sH5ACbIo4n6dIpMYwFymJel5PmF0MAMysG\nZgELoo0kcj8DrgNaow4kBUwEqoG7w6q1u8wsN+qgouDuFcCPgI3AZmCXuz8bbVTJl06JQTows0HA\nI8CX3X131PFExczeC1S5+8KoY0kR/YDZwB3uPguoBdKyTc7MhhLULEwExgC5ZvaRaKNKvnRKDBXA\nuJjXReG6tGRm/QmSwgPuPj/qeCJ2MvA+M1tPUMX4bjO7P9qQIlUOlLt7WynyYYJEkY7OAta5e7W7\nNwHzgZMijinp0ikx/BuYbGYTzSyLoAHp8YhjioSZGUH98evu/pOo44mau9/g7kXuXkzwd/FXd+/z\n3wr3x90rgTIzmxKuOhNYGWFIUdoInGBmOeH/zZmkQUN8v6gD6Cnu3mxmnweeIbiz4LfuviLisKJy\nMnAlsMzMloTrvunuT0UYk6SWLwAPhF+i1gIfizieSLj7AjN7GFhEcDffYtJgaAwNiSEiIu2kU1WS\niIjEQYlBRETaUWIQEZF2lBhERKQdJQYREWlHiUGSxsyuNrM9CT7mejP7WoKPmfA4RXozJQbpkpnd\nY2YePprMbK2Z/SiO8XP+AExKcDjHAr9M8DHjYmYzzewPZlZpZg3h8O33mNlRUcSTqpRoez8lBonX\nc8Boggv9t4HPAv+1v43NrL+773X3qkQGEQ5NUJfIY8YjHE9pATCIoHPgVIJe0puB23o6HpFkUmKQ\neDW4e6W7l7n7g8D9wMUAZnZ6WJo438z+ZWaNwLkdvzma2c3hZCdXmNlbZlZjZo+aWUHsiczsKjNb\nFn4r32Jm98a8164qKTzv583sT2ZWZ2YbOg5yZma3hRM07Q33v707k62YWQ5wN/CMu1/g7n9x93Xu\nXuruNwAfjtn2XWa2wMzqw9h/GvYebnv/b2Z2h5n92My2m1m1mX3JzAaY2S/MbKeZbTSzK2P2KQ5/\nzg+Z2UvhsVeZ2Tkd4ozn3L80sx+Y2VYzqwpLfhkx22SZ2Q/NrDz8PP9tZufGvN/2uz4zPFedmZWa\n2ey298PPKjemlHlz+N4lZvZa+HvYbmZ/N7NR8f4epOcoMcjBqgcGdFj3Q4LSxFT2P4x3MfAB4P3A\nOQRDfn+/7U0z+xTwvwQXl6OA9wCvdRHLdwnGvZpJMFzBfWZWEvN+LfBxYBpBSecK4FtdHDPWuUAB\n+ykZtM1VYMGkLn8mGDZhFnAN8EHg1g67fBioAY4Pj/kz4FHgDaAEuBe4y8xGd9jvduC/w5/zL8Bj\n4Tm7e+5mgoHgPg98meD30eZu4DTgQ8CRYSxPmNkxHY5zK8GIq7OBbQTDZxjwSnjMOoIS5mjgR2ZW\nSDBA4b0Ev4d3Af+HpCZ310OPAz6Ae4AnY14fR3Ax+EP4+nTAgbkd9rsa2BPz+maChDIkZt23CCZQ\nantdDtx2gFjWA1+Lee3Arzts8xxw/wGO8ekO52wXZyfbXxeeZ2gXn9P3gTVARodjNwA54eu/Aa/G\nvG8Ecx88HrOuP9AIXBq+Lg7P/62YbTIIEsl/Huy5w3V/Ae4Klw8jmI9ifIdtHgV+2eF3fW7M+yeH\n64r293kSJBAHJkT996xH14+0GURPDtl7wmqhfgQXrscIBlqLVRrHcTa4+66Y15uAkQBmNpJg8qTn\nuxnbq528vqDthZldSvAt9nCCNoLM8BEvi3O7acA/3T12sp+XgKzw3G0ln30lIHd3M6sClsWsazKz\nHYSfS4xXY7ZpNbMFBNPUHtS5Q/s+f4KLtwErgy//+wwA/tphv9jjtM1oNpIgsXdmKUHCXm5mz4bL\nD7t79X62lwipKkni9SJBFcYUINvdL/F3NizXxnGcpg6vnST+HZrZCQRVGM8AFxJUs3ybILnF643w\nedohhBI7WmVnn0EyP5euzt12nozw9bEEv+u2xzSCqrhYscdpO/5+43X3FoKqw3MIkso1wJpOqqgk\nBSgxSLzq3P1Nd9/gwYQlCRcmmgqCMe+744ROXreNmX8yUOHut7j7v919DTChm8d/FtjKfmYxM7P8\ncPF1grH7Y/+vTiGoFnqrm+fszL6fM6zPP463f85EnHsxQYmhMPxdxz66M6lVI52UyDzwqrt/lyD5\nbKJ9+4akCFUlSar5PvBTM9sC/Ilg8vUz3f3HB9jnEjP7N0Ed+qUEieX48L03gLFm9mGCqphzCRpl\n4+butWb2CWCemf2JoLF4DTCMoBF9NkHV1S8Jqqx+aWY/J7i19zbg/3librH9jJm9QVDt9FmCBHdH\n+N4hn9vd3zCzB4B7zOyrBHMQDCNoV1jr8c/0tx7INrOzCZJNHXA0wWxozwBbCEpu40jfCYBSmkoM\nklLc/Q7gc8AngeXA08CMLna7GZhLUEXxGeBj7v7v8HhPEPS3+Fn4/tnAdw4irseAEwkucvcDq4F5\nBPX33w63qQDOI7joLQF+C/wO+GZ3z7cf1wNfIaivfw/wfncvT/C5P0ZwZ9LtwCrgSYI7iDbEewB3\nfwX4VXj+aoLG+10EpbcnCZLqj4Fb3D2dp1BNWZqoR3o1M3PgMnd/OOpYksXMioF1wLHuHk8Dv8gh\nUYlBRETaUWIQEZF2VJUkIiLtqMQgIiLtKDGIiEg7SgwiItKOEoOIiLSjxCAiIu38f3mUfaGkIZ9w\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xe5d7390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
    "   svd_solver='auto', tol=0.0, whiten=False)\n",
    "X_pca = pca.fit(X).transform(X) # Create a matrix X that contains the  first 5 principal components of the previous X (df_arranged.values)\n",
    "scree_var = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "plt.plot(scree_var)\n",
    "plt.title('Scree Plot',fontsize = 18)\n",
    "plt.xlabel('Principal Components', fontsize=14)\n",
    "plt.ylabel('% Variance Explained', fontsize=14)\n",
    "pca.get_covariance() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though 10 components will explain about 90% of the data, there isn't a defined \"elbow\". On the other hand, if we look at the scree plot above, it is evident that at component 2 there is a sharp elbow and at component 4 there is a slight one that defines the route followed by the line. With 4 components  little over 80% of the variance in the model is explained. That is the number of components which we will select for our data, as shown in the next cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.46489313e+00,   2.76210520e-01,  -5.84382124e-04, ...,\n",
       "          2.63542896e-03,  -1.11752823e-03,   4.60473349e-02],\n",
       "       [  2.76210520e-01,   4.46607227e+00,  -5.94757942e-04, ...,\n",
       "          2.64395990e-03,  -1.13796762e-03,   4.61941934e-02],\n",
       "       [ -5.84382124e-04,  -5.94757942e-04,   4.18927737e+00, ...,\n",
       "         -8.53094522e-06,   2.04393907e-05,  -1.46858489e-04],\n",
       "       ..., \n",
       "       [  2.63542896e-03,   2.64395990e-03,  -8.53094522e-06, ...,\n",
       "          4.18929314e+00,  -1.65064003e-05,   4.56145069e-04],\n",
       "       [ -1.11752823e-03,  -1.13796762e-03,   2.04393907e-05, ...,\n",
       "         -1.65064003e-05,   4.18930726e+00,  -2.84060196e-04],\n",
       "       [  4.60473349e-02,   4.61941934e-02,  -1.46858489e-04, ...,\n",
       "          4.56145069e-04,  -2.84060196e-04,   4.19722511e+00]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEeCAYAAACOtbLLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9c7ff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(copy=True, iterated_power='auto', n_components=4, random_state=None,\n",
    "   svd_solver='auto', tol=0.0, whiten=False)\n",
    "X_pca = pca.fit(X).transform(X) # Create a matrix X that contains the  first 5 principal components of the previous X (df_arranged.values)\n",
    "scree_var = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "plt.plot(scree_var)\n",
    "plt.title('Scree Plot',fontsize = 18)\n",
    "plt.xlabel('Principal Components', fontsize=14)\n",
    "plt.ylabel('% Variance Explained', fontsize=14)\n",
    "pca.get_covariance() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of an additional new response variable for Classifcation Task 2 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of new response variable for task 2\n",
    "\n",
    "In this case, our interest resides in predicting the time in which accidents where fatalities are present happen. Therefore, for this task, first, we will create a new dataframe named **df_task2** for which we will choose the attribute **HOUR** from our \"cleaned\" dataframe called df_reduced as our response variable. Then, we will turn the attribute **HOUR** into a binary one, in which:\n",
    "\n",
    "- HOUR <= 12   = 0 (Daytime)\n",
    "- Hour > 13  = 1 (Nightime)\n",
    "\n",
    "After this operation is performed, then attribute **HOUR** will be dropped from **df_task2**.\n",
    "\n",
    "Second, the response variable we used for **TASK 1**, **FATALITIES**, will again derived from attribute **FATALS** into a binary variable with class 0 = single fatality and class 1 = multiple fatalities, and then will be included with the rest of the explanatory variables. **FATALS** then, will be also dropped from **df_task2**.\n",
    "\n",
    "Third, we will split the explanatory variables and the response variable into two different arrays named: *X_task2* and *y_task2*. \n",
    "\n",
    "Finally, we will perform PCA on this new data considering that we still have 103 columns after two attributes were dropped.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    15150\n",
       "0    13803\n",
       "Name: HR, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create new binary HR derived from HOUR\n",
    "df_task2 = df_reduced\n",
    "df_task2['HR'] = 0\n",
    "df_task2['HR'][df_task2['HOUR'] <= 12] = 0\n",
    "df_task2['HR'][df_task2['HOUR'] > 13 ] = 1\n",
    "df_task2['HR'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response variable **HR** isn't as imbalanced as **FATALITIES**--response for Task 1. Class 0 has 6,103 more observations than class 1. Not a dramatic or concerning difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop HOUR from dataframe\n",
    "df_task2.drop('HOUR', axis= 1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26795\n",
       "1     2158\n",
       "Name: FATALITIES, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create new binary attribute FATALITIES from FATALS\n",
    "df_task2['FATALITIES'] = 0\n",
    "df_task2['FATALITIES'][df_task2['FATALS'] == 1] = 0\n",
    "df_task2['FATALITIES'][df_task2['FATALS'] > 1] = 1\n",
    "df_task2['FATALITIES'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop FATALS from dataframe\n",
    "df_task2.drop('FATALS', axis= 1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'HR' in df_task2:\n",
    "    y_task2 = df_task2['HR'].values\n",
    "    del df_task2['HR']\n",
    "    X_task2 = df_task2.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction for Task 2: PCA\n",
    "\n",
    "We deleted attributes **HOUR** and **FATALS**--from which **HR** and **FATALITIES** were derived-- from the dataframe **df_task2**, and then split the dataframe into an two arrays: One, storing all the explanatory variables called X_task2 and another containing the response variable for **Task 2**, **HR**, named **y_task2**, we ended up with 103 columns. \n",
    "We started PCA conservatively with 5 components to see if those will be appropriate for our new data. After looking at the scree plot below, we realized that even though 5 components will help explain 85% of the variance the last noticeable \"elbow\" occurred at 3 components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.53266194e+00,   3.17609832e-01,  -4.66107796e-04, ...,\n",
       "          2.87722219e-03,  -9.58186188e-04,   2.62260826e-02],\n",
       "       [  3.17609832e-01,   3.53362198e+00,  -4.93934873e-04, ...,\n",
       "          2.88968425e-03,  -1.01380176e-03,   2.61900546e-02],\n",
       "       [ -4.66107796e-04,  -4.93934873e-04,   3.21554604e+00, ...,\n",
       "         -1.24620615e-05,   5.56155712e-05,   3.60279780e-05],\n",
       "       ..., \n",
       "       [  2.87722219e-03,   2.88968425e-03,  -1.24620615e-05, ...,\n",
       "          3.21554681e+00,  -2.51364536e-05,   2.15309621e-04],\n",
       "       [ -9.58186188e-04,  -1.01380176e-03,   5.56155712e-05, ...,\n",
       "         -2.51364536e-05,   3.21562937e+00,   6.96974512e-05],\n",
       "       [  2.62260826e-02,   2.61900546e-02,   3.60279780e-05, ...,\n",
       "          2.15309621e-04,   6.96974512e-05,   3.21789186e+00]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEeCAYAAACOtbLLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc78dbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
    "   svd_solver='auto', tol=0.0, whiten=False)\n",
    "X_pca_task2 = pca.fit(X_task2).transform(X_task2) # Create a matrix X that contains the  first 5 principal components of the previous X (df_arranged.values)\n",
    "scree_var = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "plt.plot(scree_var)\n",
    "plt.title('Scree Plot',fontsize = 18)\n",
    "plt.xlabel('Principal Components', fontsize=14)\n",
    "plt.ylabel('% Variance Explained', fontsize=14)\n",
    "pca.get_covariance() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we tried PCA with 3 components, and decided that those were appropriate for our data, since those will help explain a little over 80% of the variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.53969454e+00,   2.72986790e-01,  -5.13732553e-04, ...,\n",
       "          2.50631874e-03,  -1.04945734e-03,   2.22208292e-02],\n",
       "       [  2.72986790e-01,   4.54073032e+00,  -5.22053680e-04, ...,\n",
       "          2.51327460e-03,  -1.06612027e-03,   2.22425243e-02],\n",
       "       [ -5.13732553e-04,  -5.22053680e-04,   4.26722980e+00, ...,\n",
       "         -6.95586826e-06,   1.66629271e-05,  -2.16951302e-05],\n",
       "       ..., \n",
       "       [  2.50631874e-03,   2.51327460e-03,  -6.95586826e-06, ...,\n",
       "          4.26724521e+00,  -1.41077231e-05,   1.98268422e-04],\n",
       "       [ -1.04945734e-03,  -1.06612027e-03,   1.66629271e-05, ...,\n",
       "         -1.41077231e-05,   4.26725485e+00,  -4.52409038e-05],\n",
       "       [  2.22208292e-02,   2.22425243e-02,  -2.16951302e-05, ...,\n",
       "          1.98268422e-04,  -4.52409038e-05,   4.26908914e+00]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEeCAYAAACOtbLLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcaa9128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
    "   svd_solver='auto', tol=0.0, whiten=False)\n",
    "X_pca_task2 = pca.fit(X_task2).transform(X_task2) # Create a matrix X that contains the  first 5 principal components of the previous X (df_arranged.values)\n",
    "scree_var = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "plt.plot(scree_var)\n",
    "plt.title('Scree Plot',fontsize = 18)\n",
    "plt.xlabel('Principal Components', fontsize=14)\n",
    "plt.ylabel('% Variance Explained', fontsize=14)\n",
    "pca.get_covariance() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>[5 points]</b>\n",
    "*Describe the final dataset that is used for classification/regression (include a\n",
    "description of any newly formed variables you created).* </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we explained before we ended up with two dataframes: **df_task1** and **df_task2**. The only difference between the two resides in the response variables chosen: **df_task1** has **y**, which contains our binary **FATALITIES** where class 0 = single death and class 1 = multiple fatalities in an accident; **df_task2**, on the other hand, has **y_task2**, which contains the binary **HR**, in which class 0 = less than 12 (Daytime) and class 1 = greater or equal to 12 (Nightime). \n",
    "\n",
    "To get to these final data sets, we removed those attributes that did not add any useful information, could be derived from other attributes or simply added extra not needed noise to our response variables whether **FATALITIES** or **HOUR**. Those attributes that were deleted from the original data set are as follows:\n",
    "<li>**ST_CASE** </li>\n",
    "<li>**COUNTY**  </li>\n",
    "<li>**SP_JUR**  </li>\n",
    "<li>**RD_OWNER**</li>\n",
    "<li>**RUR_URB** </li>\n",
    "<li>**LONGITUD**</li>\n",
    "<li>**LATITUDE**</li>\n",
    "<li>**STATE**   </li>\n",
    "<li>**FUNC_SYS** </li>\n",
    "<li> **TYP_INT** </li>\n",
    "<li> **NHS**     </li>\n",
    "<li> **TWAY_ID** </li>\n",
    "<li> **DATETIME**</li>\n",
    "\n",
    "\n",
    "Then, we defined a cutoff value for some other attributes, like we explained when the operation was performed above, in which anything that has less than 10 observations was removed as those observations were not representative, and could have been considered as outliers. \n",
    "\n",
    "We did one-hot encoding of our categorical variables, which resulted in a dramatic increase of the number of columns (from 21 to 104).\n",
    "\n",
    "#### df_task1\n",
    "\n",
    "For **df_task1** we created a new ordinal attribute named **HR_RANGE**, from the original attribute **HOUR**, with six levels grouping the hours of a day, that combined with other attributes could help us determine the factors that influence that an accident could end up in a fatality (or fatalities) at a certain period of time. \n",
    "\n",
    "We created a new variable with two binary classes called **FATALITIES** , which was derived from our original response variable **FATALS** (a continuous variable describing the number of fatalities in a driving accident). **FATALITIES** later became **y**.  \n",
    "\n",
    "We divided our dataframe into **X** (containing all the explanatory variables) and **y** (containing the response). \n",
    "\n",
    "Due to the high number of columns (104), we performed a dimensionality reduction that result in an array called **X_pca** with 4 components explaining over 80% of the variability of the data. We will use this array, along with the response variable to perform the classification and regression tasks that follow. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_task2 \n",
    "\n",
    "For **df_task2**, overall the attributes from **df_task1** remained untouched except from **HR_RANGE** which was dropped and replaced by the original attribute **HOUR**. We included **FATALITIES**--our response variable for Task 1--as an explanatory for Task 2 and created a new binary response variable **HR**--derived from **HOUR**, which we dropped afterwards--with classes 0, representing Daytime hours and 1, reflecting nighttime ones. \n",
    "\n",
    "As for **df_task1**, for **df_task2** we separated the response variable **HR** into an array that was named **y_task2** and placed all the explanatory variables, which in this case contained was used to be the response for **df_task1**, **FATALITIES**, within the explanatory variables array called **X_task2**. \n",
    "\n",
    "For **df_task2** we ended up with 103 columns, so dimensionality reduction was performed leaving aside **y_task2**, then obtaining 3 components--contained in the array called **X_pca_task2** that explain about 80% of the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Choose and explain your evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal using the FARS dataset to classify the fatalities from fatal accidents and the hour in which the accidents occurred.\n",
    "\n",
    "Using both the Fatalities and Hours response variables becomes a multiclass problem. The Fatalities (Fatalaties) response variable is heavily unbalanced with 26795 accidents with class 0 (one fatality), and 2158 accidents with class 2 (multiple fatalities).  The Hour (HR) response variable provides a more balanced set with 17528 class 1 (accidents occuring in the nightime), and 11425 class 0 (accidents occuring in the daytime).\n",
    "\n",
    "Accuracy measures a fraction of the classifierâ€™s predictions that are correct, that is the number of correct assessments divided by the number of all assessments â€“ (TN + TP)/(TN + TP + FN + FP) (http://librimind.com). \n",
    "\n",
    "Accuracy will be an important measurement with our logistic regression model with the understanding that the cost parameter needs to be adjusted to reduce misclassifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Choose the method you will use for dividing up your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a stratified shuffle split to split the data into train/test sets.  This cv object is a merge of StratifiedKFold and ShuffleSplit.  It will return stratified randomized folds, which are made by by preserving the percentage of samples for each class (http://scikit-learn.org).\n",
    "\n",
    "After reducing the dimensions of our explanatory variables (X) data into four principal components (X_pca) and three principal components (X_pca_task2), we split the data into a training set, containing 80% of the data, and a test set containing the remaining 20%. We wanted to ensure that the subgroups of data that were split into training and testing were not only random but also proportional.  We chose the 80:20 split because it is popular and to have more training data to reduce variance.\n",
    "\n",
    "We chose a cross-validation with 10 folds because it provides better estimates of the error, and it helps us determine the predictive power of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and Training and Testing Split\n",
    "This cv will be used for the Fatalaties response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=10, random_state=None, test_size=0.2,\n",
      "            train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Set cross-validation parameters, which is what we will use to determine the predictve power of our model.\n",
    "num_cv_iterations = 10 # we ara choosing 10 folds because it provides the best estimate of error.\n",
    "num_instances = len(y)\n",
    "\n",
    "# Now split data into Training (80%) and Test (20%) data sets\n",
    "\n",
    "cv_object = StratifiedShuffleSplit(n_splits = num_cv_iterations, test_size = 0.2) # we use stratified to make sure that \n",
    "# train and test data contains important information on both\n",
    "\n",
    "print(cv_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Create three different classification/regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic Regression with Fatalities as response variable and Logistic Regression with Hour as response variable\n",
    "2. Classification 1 with Fatalities as response variable (RF and KNN)\n",
    "3. Classification 2 with Hour as response variable (RF and KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Fatalities as Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('====Iteration', 0, ' ====')\n",
      "('accuracy', 0.9216024866171646)\n",
      "('confusion matrix\\n', array([[5327,   32],\n",
      "       [ 422,   10]]))\n",
      "('====Iteration', 1, ' ====')\n",
      "('accuracy', 0.92384734933517532)\n",
      "('confusion matrix\\n', array([[5340,   19],\n",
      "       [ 422,   10]]))\n",
      "('====Iteration', 2, ' ====')\n",
      "('accuracy', 0.92332930409255742)\n",
      "('confusion matrix\\n', array([[5336,   23],\n",
      "       [ 421,   11]]))\n",
      "('====Iteration', 3, ' ====')\n",
      "('accuracy', 0.92384734933517532)\n",
      "('confusion matrix\\n', array([[5345,   14],\n",
      "       [ 427,    5]]))\n",
      "('====Iteration', 4, ' ====')\n",
      "('accuracy', 0.92263857710240027)\n",
      "('confusion matrix\\n', array([[5334,   25],\n",
      "       [ 423,    9]]))\n",
      "('====Iteration', 5, ' ====')\n",
      "('accuracy', 0.9216024866171646)\n",
      "('confusion matrix\\n', array([[5329,   30],\n",
      "       [ 424,    8]]))\n",
      "('====Iteration', 6, ' ====')\n",
      "('accuracy', 0.92263857710240027)\n",
      "('confusion matrix\\n', array([[5336,   23],\n",
      "       [ 425,    7]]))\n",
      "('====Iteration', 7, ' ====')\n",
      "('accuracy', 0.92315662234501816)\n",
      "('confusion matrix\\n', array([[5343,   16],\n",
      "       [ 429,    3]]))\n",
      "('====Iteration', 8, ' ====')\n",
      "('accuracy', 0.92073907787946818)\n",
      "('confusion matrix\\n', array([[5323,   36],\n",
      "       [ 423,    9]]))\n",
      "('====Iteration', 9, ' ====')\n",
      "('accuracy', 0.92350198584009668)\n",
      "('confusion matrix\\n', array([[5339,   20],\n",
      "       [ 423,    9]]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# Create logistic regrssion object\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) #We will start with a cost of 1.0 and then compare\n",
    "# with a model with a lower cost later on\n",
    "\n",
    "#We decided to use this code from notebook 04 because it is clear what it does, and how it does it. \n",
    "iter_num=0 \n",
    "for train_indices, test_indices in cv_object.split(X_pca,y): \n",
    "    X_train = X_pca[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X_pca[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    lr_clf.fit(X_train,y_train)  # here we use our train data to fit the logistic regression\n",
    "    y_hat = lr_clf.predict(X_test) # and we use test data to predict\n",
    "\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:  Our accuracy is high, which may be due to our response variable having a greater number of zeros (more than 20,000) than ones (about 1,600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Fatalaties response variable change parameter(s)\n",
    "\n",
    "Changing the cost parameter using three different values (100, 1, 0.01) and the penalties L1 and L2 to determine if the false negatives and the accuracy changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=100.00\n",
      "Sparsity with L1 penalty: 0.00%\n",
      "score with L1 penalty: 0.9234\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.9234\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.92350198584009668)\n",
      "('confusion matrix\\n', array([[5339,   20],\n",
      "       [ 423,    9]]))\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.92332930409255742)\n",
      "('confusion matrix\\n', array([[5339,   20],\n",
      "       [ 424,    8]]))\n",
      "C=1.00\n",
      "Sparsity with L1 penalty: 0.00%\n",
      "score with L1 penalty: 0.9234\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.9234\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.92350198584009668)\n",
      "('confusion matrix\\n', array([[5339,   20],\n",
      "       [ 423,    9]]))\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.92332930409255742)\n",
      "('confusion matrix\\n', array([[5339,   20],\n",
      "       [ 424,    8]]))\n",
      "C=0.01\n",
      "Sparsity with L1 penalty: 0.00%\n",
      "score with L1 penalty: 0.9237\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.9232\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.92367466758763594)\n",
      "('confusion matrix\\n', array([[5343,   16],\n",
      "       [ 426,    6]]))\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.92367466758763594)\n",
      "('confusion matrix\\n', array([[5341,   18],\n",
      "       [ 424,    8]]))\n"
     ]
    }
   ],
   "source": [
    "#Set regularization parameter\n",
    "for i, C in enumerate((100, 1, 0.01)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    clf_l1_LR.fit(X_train,y_train)\n",
    "    clf_l2_LR.fit(X_train,y_train)\n",
    "\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n",
    "    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(X_train,y_train))\n",
    "    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n",
    "    print(\"score with L2 penalty: %.4f\" % clf_l2_LR.score(X_train,y_train))\n",
    "    \n",
    "    y_hat_l1 = clf_l1_LR.predict(X_test)\n",
    "    y_hat_l2 = clf_l2_LR.predict(X_test)\n",
    "    \n",
    "    acc = mt.accuracy_score(y_test,y_hat_l1)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat_l1)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "    acc = mt.accuracy_score(y_test,y_hat_l2)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat_l2)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether the cost parameter is high or low (ie. c = 100 or c = 0.01) the accuracy is about the same 92.2%; however, in the confusion matrix we see that when cost is low (ie. c = 0.01) we run the risk of misclassifications of class 1 due to the imbalance of the response variable where Class 1 responses may be ignored because they make up only about 7% of the fatalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and Training and Testing Split 2\n",
    "\n",
    "This cv will be used for the **HR** response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=10, random_state=None, test_size=0.2,\n",
      "            train_size=None)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the train/test differently due to the more balanced data for the Hour response variable\n",
    "\n",
    "# Set cross-validation parameters, which is what we will use to determine the predictve power of our model.\n",
    "num_cv_iterations = 10 # we ara choosing 10 folds because it provides the best estimate of error.\n",
    "num_instances = len(y_task2)\n",
    "\n",
    "# Now split data into Training (70%) and Test (20%) data sets\n",
    "\n",
    "cv_object2 = StratifiedShuffleSplit(n_splits = num_cv_iterations, test_size = 0.2) # we use stratified to make sure that \n",
    "# train and test data contains important information on both\n",
    "\n",
    "print(cv_object2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with HR as Response Variable\n",
    "\n",
    "Since we are using our other response variable, we created a new logistic regression object called \"lr_clf2\" we will be able to fit our training data, which will be now stored in new variables: X_train2 containing our three chosen components as the explanatory variable and y_train2 containing the response variable **HR**, and use the test data, also contained in new variables: X_test2 (explanatory) and y_test2 (response), which we will use to predict by creating an object called y_hat2.\n",
    "\n",
    "Also, we will use the test response variable y_test2 and the object y_hat2 to calculate the accuracy score and the confusion matrices based on the 10 folds that we selected for the cross validation object called cv_object2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('====Iteration', 0, ' ====')\n",
      "('accuracy', 0.52443446727680887)\n",
      "('confusion matrix\\n', array([[ 176, 2585],\n",
      "       [ 169, 2861]]))\n",
      "('====Iteration', 1, ' ====')\n",
      "('accuracy', 0.52339837679157308)\n",
      "('confusion matrix\\n', array([[   7, 2754],\n",
      "       [   6, 3024]]))\n",
      "('====Iteration', 2, ' ====')\n",
      "('accuracy', 0.52270764980141604)\n",
      "('confusion matrix\\n', array([[ 358, 2403],\n",
      "       [ 361, 2669]]))\n",
      "('====Iteration', 3, ' ====')\n",
      "('accuracy', 0.52339837679157308)\n",
      "('confusion matrix\\n', array([[   2, 2759],\n",
      "       [   1, 3029]]))\n",
      "('====Iteration', 4, ' ====')\n",
      "('accuracy', 0.51942669659816953)\n",
      "('confusion matrix\\n', array([[  96, 2665],\n",
      "       [ 118, 2912]]))\n",
      "('====Iteration', 5, ' ====')\n",
      "('accuracy', 0.5228803315489553)\n",
      "('confusion matrix\\n', array([[  14, 2747],\n",
      "       [  16, 3014]]))\n",
      "('====Iteration', 6, ' ====')\n",
      "('accuracy', 0.52305301329649456)\n",
      "('confusion matrix\\n', array([[ 291, 2470],\n",
      "       [ 292, 2738]]))\n",
      "('====Iteration', 7, ' ====')\n",
      "('accuracy', 0.52408910378173024)\n",
      "('confusion matrix\\n', array([[   6, 2755],\n",
      "       [   1, 3029]]))\n",
      "('====Iteration', 8, ' ====')\n",
      "('accuracy', 0.52426178552926961)\n",
      "('confusion matrix\\n', array([[  39, 2722],\n",
      "       [  33, 2997]]))\n",
      "('====Iteration', 9, ' ====')\n",
      "('accuracy', 0.5228803315489553)\n",
      "('confusion matrix\\n', array([[   7, 2754],\n",
      "       [   9, 3021]]))\n"
     ]
    }
   ],
   "source": [
    "# Create logistic regrssion object\n",
    "lr_clf2 = LogisticRegression(penalty='l2', C=1.0, class_weight=None) #We will start with a cost of 1.0 and then compare\n",
    "# with a model with a lower cost later on\n",
    "\n",
    "#We decided to use this code from notebook 04 because it is clear what it does, and how it does it. \n",
    "iter_num=0 \n",
    "for train_indices, test_indices in cv_object2.split(X_pca_task2,y_task2): \n",
    "    X_train2 = X_pca_task2[train_indices]\n",
    "    y_train2 = y_task2[train_indices]\n",
    "    \n",
    "    X_test2 = X_pca_task2[test_indices]\n",
    "    y_test2 = y_task2[test_indices]\n",
    "    \n",
    "    lr_clf2.fit(X_train2,y_train2)  # here we use our train data to fit the logistic regression\n",
    "    y_hat2 = lr_clf2.predict(X_test2) # and we use test data to predict\n",
    "\n",
    "    acc = mt.accuracy_score(y_test2,y_hat2)\n",
    "    conf = mt.confusion_matrix(y_test2,y_hat2)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting enough, iterations 6, 7 and 9 provided the higher number of True Positives, False Positives, False Negatives and True Negatives, whereas the rest of the iterations were visibly low in the number of True Positives and False Positives. We suppose these were less \"sensitive\" and had less specificity than the rest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Hour response variable change parameter(s)\n",
    "\n",
    "Changing the cost parameter to determine if the false negatives and the accuracy changes when the cost is high, medium or low. For this model we are using two different penalties: L1 and L2 along with three different values for C (100, 1, 0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=100.00\n",
      "Sparsity with L1 penalty: 0.00%\n",
      "score with L1 penalty: 0.5236\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.5235\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.5228803315489553)\n",
      "('confusion matrix\\n', array([[   7, 2754],\n",
      "       [   9, 3021]]))\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.5228803315489553)\n",
      "('confusion matrix\\n', array([[   7, 2754],\n",
      "       [   9, 3021]]))\n",
      "C=1.00\n",
      "Sparsity with L1 penalty: 0.00%\n",
      "score with L1 penalty: 0.5236\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.5235\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.5228803315489553)\n",
      "('confusion matrix\\n', array([[   7, 2754],\n",
      "       [   9, 3021]]))\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.5228803315489553)\n",
      "('confusion matrix\\n', array([[   7, 2754],\n",
      "       [   9, 3021]]))\n",
      "C=0.01\n",
      "Sparsity with L1 penalty: 0.00%\n",
      "score with L1 penalty: 0.5237\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.5235\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.52339837679157308)\n",
      "('confusion matrix\\n', array([[   6, 2755],\n",
      "       [   5, 3025]]))\n",
      "('====Iteration', 10, ' ====')\n",
      "('accuracy', 0.5228803315489553)\n",
      "('confusion matrix\\n', array([[   7, 2754],\n",
      "       [   9, 3021]]))\n"
     ]
    }
   ],
   "source": [
    "#Set regularization parameter\n",
    "for i, C in enumerate((100, 1, 0.01)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR_2 = LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    clf_l2_LR_2 = LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    clf_l1_LR_2.fit(X_train2,y_train2)\n",
    "    clf_l2_LR_2.fit(X_train2,y_train2)\n",
    "\n",
    "    coef_l1_LR_2 = clf_l1_LR_2.coef_.ravel()\n",
    "    coef_l2_LR_2 = clf_l2_LR_2.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR_2 = np.mean(coef_l1_LR_2 == 0) * 100\n",
    "    sparsity_l2_LR_2 = np.mean(coef_l2_LR_2 == 0) * 100\n",
    "\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR_2)\n",
    "    print(\"score with L1 penalty: %.4f\" % clf_l1_LR_2.score(X_train2,y_train2))\n",
    "    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR_2)\n",
    "    print(\"score with L2 penalty: %.4f\" % clf_l2_LR_2.score(X_train2,y_train2))\n",
    "    \n",
    "    y_hat_l1_2 = clf_l1_LR_2.predict(X_test2)\n",
    "    y_hat_l2_2 = clf_l2_LR_2.predict(X_test2)\n",
    "    \n",
    "    acc = mt.accuracy_score(y_test2,y_hat_l1_2)\n",
    "    conf = mt.confusion_matrix(y_test2,y_hat_l1_2)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "    acc = mt.accuracy_score(y_test2,y_hat_l2_2)\n",
    "    conf = mt.confusion_matrix(y_test2,y_hat_l2_2)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When C = 100 there is no difference between L1 and L2, as both provide a score 0f 0.53. When C = 1.0, there is an insignificant difference of 0.01 between L1 and L2, where L1 score is of 0.54, and L2 score, 0.53. When C = 0.01, interesting enough L2 displays a score of 0.53, while L1 score if of 0.52, still a minimal difference of 0.01. \n",
    "\n",
    "L2 penalty offers a greater sparsity without reducing the predictive power of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Task 1 - Random Forest and KNeighbors with Fatalities as Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: KNeighbors with Fatalities as Response Variable\n",
    "We run ten iterations in order to compare the mean average of accuracy between runs (parameter changes) and between classification models in section 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KNN accuracy', 0.92380755016751281)\n",
      "('KNN accuracy', 0.92252961696542668)\n",
      "('KNN accuracy', 0.92149345490968115)\n",
      "('KNN accuracy', 0.92087175767623386)\n",
      "('KNN accuracy', 0.91952474700376474)\n",
      "('KNN accuracy', 0.91900666597589198)\n",
      "('KNN accuracy', 0.91859220115359375)\n",
      "('KNN accuracy', 0.91835043000725314)\n",
      "('KNN accuracy', 0.91779781024418883)\n",
      "('KNN accuracy', 0.918005042655338)\n",
      "('average accuracy:', 0.9199979276758885)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# KNeighbors with Fatalities\n",
    "\n",
    "# These will be used for Classifcation RF & KNN \n",
    "X = X_pca\n",
    "yhat = np.zeros(y.shape)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "iter_num=0\n",
    "total_acc=0\n",
    "for train, test in cv_object.split(X,y):\n",
    "    clf_KNN.fit(X[train],y[train])\n",
    "    yhat[test] = clf_KNN.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    total_acc+=total_accuracy\n",
    "    print ('KNN accuracy', total_accuracy)\n",
    "    iter_num+=1\n",
    "\n",
    "acc_avg=total_acc/10\n",
    "print ('average accuracy:',acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Random Forest with Fatalaties as Response Variable\n",
    "We run ten iterations in order to compare the mean average of accuracy between runs (parameter changes) and between classification models in section 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RF accuracy', 0.91686526439401794)\n",
      "('RF accuracy', 0.91510378889925048)\n",
      "('RF accuracy', 0.91382585569716435)\n",
      "('RF accuracy', 0.91261699996546131)\n",
      "('RF accuracy', 0.91278969364141882)\n",
      "('RF accuracy', 0.91334231340448313)\n",
      "('RF accuracy', 0.9123061513487376)\n",
      "('RF accuracy', 0.91202984146720545)\n",
      "('RF accuracy', 0.91209891893758854)\n",
      "('RF accuracy', 0.91154629917452423)\n",
      "('average accuracy:', 0.91325251269298524)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest with Fatalities\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf_RF = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "iter=0\n",
    "total_acc=0\n",
    "for train, test in cv_object.split(X,y):\n",
    "    clf_RF.fit(X[train],y[train])\n",
    "    yhat[test] = clf_RF.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    total_acc+=total_accuracy\n",
    "    print ('RF accuracy', total_accuracy)\n",
    "    iter_num+=1\n",
    "    \n",
    "acc_avg=total_acc/10\n",
    "print ('average accuracy:',acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: KNN parameter change\n",
    "Change n_neighbors to 25 (keeping it odd) and checking to see if scaling the data set makes a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KNN accuracy', 0.92546540945670575)\n",
      "('KNN accuracy', 0.92556902566228028)\n",
      "('KNN accuracy', 0.92556902566228028)\n",
      "('KNN accuracy', 0.92556902566228028)\n",
      "('KNN accuracy', 0.92539633198632265)\n",
      "('KNN accuracy', 0.9253617932511311)\n",
      "('KNN accuracy', 0.92529271578074812)\n",
      "('KNN accuracy', 0.9251200221047905)\n",
      "('KNN accuracy', 0.92522363831036503)\n",
      "('KNN accuracy', 0.92522363831036503)\n",
      "('average accuracy:', 0.92537906261872693)\n"
     ]
    }
   ],
   "source": [
    "# Change n_neighbors to 25 (keeping it odd)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# KNeighbors with Fatalities\n",
    "\n",
    "# These will be used for Classifcation RF & KNN \n",
    "X = X_pca\n",
    "yhat = np.zeros(y.shape)\n",
    "\n",
    "#Scaling\n",
    "scl = StandardScaler()\n",
    "X = scl.fit_transform(X)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=25)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "iter=0\n",
    "total_acc=0\n",
    "for train, test in cv_object.split(X,y):\n",
    "    clf_KNN.fit(X[train],y[train])\n",
    "    yhat[test] = clf_KNN.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    total_acc+=total_accuracy\n",
    "    print ('KNN accuracy', total_accuracy)\n",
    "    iter_num+=1\n",
    "    \n",
    "acc_avg=total_acc/10\n",
    "print ('average accuracy:',acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: RF parameter change\n",
    "Change n_estimators to 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RF accuracy', 0.92187338099678795)\n",
      "('RF accuracy', 0.9197319794149138)\n",
      "('RF accuracy', 0.91835043000725314)\n",
      "('RF accuracy', 0.91693434186440093)\n",
      "('RF accuracy', 0.91524194384001656)\n",
      "('RF accuracy', 0.91358408455082374)\n",
      "('RF accuracy', 0.91310054225814252)\n",
      "('RF accuracy', 0.91261699996546131)\n",
      "('RF accuracy', 0.91227161261354606)\n",
      "('RF accuracy', 0.91161537664490722)\n",
      "('average accuracy:', 0.91553206921562524)\n"
     ]
    }
   ],
   "source": [
    "# Change n_estimators to 150\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf_RF = RandomForestClassifier(n_estimators = 150)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "iter=0\n",
    "total_acc=0\n",
    "for train, test in cv_object.split(X,y):\n",
    "    clf_RF.fit(X[train],y[train])\n",
    "    yhat[test] = clf_RF.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    total_acc+=total_accuracy\n",
    "    print ('RF accuracy', total_accuracy)\n",
    "    iter_num+=1\n",
    "    \n",
    "acc_avg=total_acc/10\n",
    "print ('average accuracy:',acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifcation Task 2  - Random Forest and KNeighbors with HR as Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: KNeighbors with HR as Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KNN accuracy', 0.4903464235139709)\n",
      "('KNN accuracy', 0.50060442786585158)\n",
      "('KNN accuracy', 0.50903187925258175)\n",
      "('KNN accuracy', 0.51296929506441469)\n",
      "('KNN accuracy', 0.51960073222118608)\n",
      "('KNN accuracy', 0.52295098953476327)\n",
      "('KNN accuracy', 0.52754464131523504)\n",
      "('KNN accuracy', 0.53027320139536493)\n",
      "('KNN accuracy', 0.53279452906434566)\n",
      "('KNN accuracy', 0.5352467792629434)\n",
      "('average accuracy:', 0.51813628984906557)\n"
     ]
    }
   ],
   "source": [
    "# KNeighbors with Hour\n",
    "\n",
    "# These will be used for Classifcation RF & KNN \n",
    "X = X_pca_task2\n",
    "y = y_task2\n",
    "yhat = np.zeros(y_task2.shape)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "iter=0\n",
    "total_acc=0\n",
    "for train, test in cv_object2.split(X,y):\n",
    "    clf_KNN.fit(X[train],y[train])\n",
    "    yhat[test] = clf_KNN.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    total_acc+=total_accuracy\n",
    "    print ('KNN accuracy', total_accuracy)\n",
    "    iter_num+=1\n",
    "\n",
    "acc_avg=total_acc/10\n",
    "print ('average accuracy:',acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Random Forest with HR as Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RF accuracy', 0.53828618795979688)\n",
      "('RF accuracy', 0.54063482195282009)\n",
      "('RF accuracy', 0.54405415673678026)\n",
      "('RF accuracy', 0.54640279072980347)\n",
      "('RF accuracy', 0.54695541049286778)\n",
      "('RF accuracy', 0.54719718163920839)\n",
      "('RF accuracy', 0.54885504092840121)\n",
      "('RF accuracy', 0.55054743895278557)\n",
      "('RF accuracy', 0.55009843539529579)\n",
      "('RF accuracy', 0.55089282630470071)\n",
      "('average accuracy:', 0.54639242910924601)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest with Hour\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf_RF = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "iter=0\n",
    "total_acc=0\n",
    "for train, test in cv_object2.split(X,y):\n",
    "    clf_RF.fit(X[train],y[train])\n",
    "    yhat[test] = clf_RF.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    total_acc+=total_accuracy\n",
    "    print ('RF accuracy', total_accuracy)\n",
    "    iter_num+=1\n",
    "    \n",
    "acc_avg=total_acc/10\n",
    "print ('average accuracy:',acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: KNN parameter change\n",
    "Change n_neighbors to 7 (keeping it odd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KNN accuracy', 0.49117535315856731)\n",
      "('KNN accuracy', 0.50267675197734263)\n",
      "('KNN accuracy', 0.51397091838496878)\n",
      "('KNN accuracy', 0.52312368321072078)\n",
      "('KNN accuracy', 0.52844264843021449)\n",
      "('KNN accuracy', 0.5325182191828135)\n",
      "('KNN accuracy', 0.53490139191102826)\n",
      "('KNN accuracy', 0.53800987807826472)\n",
      "('KNN accuracy', 0.54035851207128793)\n",
      "('KNN accuracy', 0.54263806859392805)\n",
      "('average accuracy:', 0.5247815424999136)\n"
     ]
    }
   ],
   "source": [
    "# Change n_neighbors to 7 (keeping it odd)\n",
    "\n",
    "# These will be used for Classifcation RF & KNN \n",
    "X = X_pca_task2\n",
    "y = y_task2\n",
    "yhat = np.zeros(y_task2.shape)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "iter=0\n",
    "total_acc=0\n",
    "for train, test in cv_object2.split(X,y):\n",
    "    clf_KNN.fit(X[train],y[train])\n",
    "    yhat[test] = clf_KNN.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    total_acc+=total_accuracy\n",
    "    print ('KNN accuracy', total_accuracy)\n",
    "    iter_num+=1\n",
    "    \n",
    "acc_avg=total_acc/10\n",
    "print ('average accuracy:',acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: RF parameter change\n",
    "Change n_estimators to 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RF accuracy', 0.54553932235001557)\n",
      "('RF accuracy', 0.54781887887265568)\n",
      "('RF accuracy', 0.54685179428729325)\n",
      "('RF accuracy', 0.54526301246848341)\n",
      "('RF accuracy', 0.54781887887265568)\n",
      "('RF accuracy', 0.54664456187614408)\n",
      "('RF accuracy', 0.54868234725244358)\n",
      "('RF accuracy', 0.54885504092840121)\n",
      "('RF accuracy', 0.54975304804338065)\n",
      "('RF accuracy', 0.55016751286567889)\n",
      "('average accuracy:', 0.54773943978171524)\n"
     ]
    }
   ],
   "source": [
    "# Change n_estimators to 200\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "clf_RF = RandomForestClassifier(n_estimators = 200)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "iter=0\n",
    "total_acc=0\n",
    "for train, test in cv_object2.split(X,y):\n",
    "    clf_RF.fit(X[train],y[train])\n",
    "    yhat[test] = clf_RF.predict(X[test])\n",
    "\n",
    "    total_accuracy = mt.accuracy_score(y, yhat)\n",
    "    total_acc+=total_accuracy\n",
    "    print ('RF accuracy', total_accuracy)\n",
    "    iter_num+=1\n",
    "    \n",
    "acc_avg=total_acc/10\n",
    "print ('average accuracy:',acc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Analyze the results using your chosen method of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb185e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def per_class_accuracy(ytrue,yhat):\n",
    "    conf = mt.confusion_matrix(ytrue,yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue,yhat, title=''):\n",
    "    acc_list = per_class_accuracy(ytrue,yhat)\n",
    "    plt.bar(range(len(acc_list)), acc_list)\n",
    "    plt.xlabel('Class value')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title+\", Total Acc=%.1f\"%(100*mt.accuracy_score(ytrue,yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    \n",
    "plot_class_acc(y,yhat,title=\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCmZmlnNSMDOz3H8Bkh5ed7WGdrsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9d38438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_class_acc(y,yhat,title=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the KNN and Random Forest performed similarly on the data set, with neither biased toward a particular class. It is at once heartening and disappointing as though clearly the models work, their performance is lackluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Discuss the advantages of each model\n",
    "#### *Discuss the advantages of each model for each classification task, if any. If there are no advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques - be sure they are appropriate for your chosen method of validation. (10 points total).*\n",
    "\n",
    "##### Background Information:\n",
    "Resources:  *Introduction to Machine Learning with Python* and [Quora](https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms).\n",
    "\n",
    "#### K - Nearest Neighbors Algorithm (KNN)\n",
    "\n",
    "The most important parameters in KNN is the number of neighbors and how the distance between points are measured. One of the strengths of KNN is that it is more simplistic and provides results that don't need to be adjusted many times.  It provides a good starting point for more complicated and advanced models.  Scaling the dataset is needed when different units are present.  Performing a PCA and avoiding very sparse datasets can improve the results of KNN. \n",
    "\n",
    "#### Random Forest Algorithm (RF)\n",
    "\n",
    "Random Forrest widely used in machine learning and it works well without a lot of heavy tuning of parameters.  It also doesn't require scaling the data.  You are able to build a RF, which is a bunch of decision trees, in parallel which allows it to work well on large datasets. However, RF is memory intensive and slow to train and predict compared to linear algorithms. In addition, they do not perform well on high dimensional, sparse data compared to Naive Bayes.  It is important to note that the parameters, such as number of estimators, is important when running RF.  A higher number of estimaters is better, but requires more time and memory. \n",
    "\n",
    "##### Our Assessment:\n",
    "We compared the LR output (first run versus second run with parameter changes), KNN output (first run versus second run with parameter changes), the RF output ((first run versus second run with parameter changes), and the KNN most accurate output with the LR most accurate output by running TTests in SAS.  Our conclusion for all comparisons were that none of them were statistically significant.   \n",
    "\n",
    "Fatalities Response Variable:\n",
    "LR1 vs LR2 (average) = T-statistic = 0.0002\n",
    "KNN1 (n_neighbors=5) vs KNN2 (n_neighbors=25) - T-statistic = 0.0019\n",
    "RF1 (n_estimators = 100) vs RF2 (n_estimators = 150) - T-statistic = 0.0009\n",
    "\n",
    "Hour Response Variable:\n",
    "LR1 vs LR2 (average) = T-statistic = 0.0009\n",
    "KNN1 (n_neighbors=3) vs KNN2 (n_neighbors=7) - T-statistic = 0.0009\n",
    "RF1 (n_estimators = 100) vs RF2 (n_estimators = 200) - T-statistic = 0.0027\n",
    "\n",
    "Most Accurate - Fatalaties Response Variable\n",
    "KNN1 (n_neighbors = 25) vs LR1 (C=1.0) - T-statistic = 0.0007\n",
    "\n",
    "Figure 1 illustrates the SAS Ouput results for T-Test between the most accurate KNN to the most accurate LR for the response variable Fatalities.\n",
    "\n",
    "![SAS_KNN_vs_LR](SAS_KNN_vs_LR.png \"SAS Output KNN vs LR Response Variable = Fatalities\") \n",
    "<p style='text-align: center;'>\n",
    "Figure 1. SAS Output\n",
    "</p>\n",
    "\n",
    "Although, the accuracy differences were not statistically different, the performance (run time) for the KNeighbors was faster than the Random Forest.\n",
    "\n",
    "Comparing the Response Variables, the Fatalaties response variable was more accurate than the Hour response variable.  Comparing the second run of KNN (n_neighbors = 25) for the Fatalities response variable mean accuracy to the second run of KNN (n_neigbors = 7) for the Hour response varialbe mean accuracy, there was a significant difference (T-statistic = 0.1731, 95% confidence) (See Figure 2).\n",
    "\n",
    "![SAS_KNNF_vs_KNNH](SAS_KNN_1_vs_KNN_2.png \"SAS Output KNN Fatalatiies vs KNN Hour\") \n",
    "<p style='text-align: center;'>\n",
    "Figure 2. SAS Output\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 - Which attributes from your analysis are most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Considering that we reduced the dimensionality of our data using PCA selecting only four principal components that later were used to split data into training and test sets, which in turn were used to pass through the logistic regression object, it seems adequate to calculate the weights of those four components that are now part of the train set (X_train) in order to decide if all of them are worth keeping and if their presence do not cause an overfitting of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Fatalities as Response Variable - weights\n",
    "\n",
    "Since our dataframe **df_task1**, from which **FATALITIES** or **y** is the response variable was reduced using PCA to only 4 components, we decided it was indispensable to get the weights of each of those components to determine their influence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1.71686021,  1.13674501,  2.78865833, -0.61995132]), 'This principal component from the training data has weight of', 0.18)\n",
      "(array([-2.72108962,  0.2404349 ,  1.79932863, -0.56378588]), 'This principal component from the training data has weight of', 0.11)\n",
      "(array([ 2.54187449, -3.82136085, -0.03280968,  0.05496253]), 'This principal component from the training data has weight of', 0.05)\n",
      "(array([ 0.59269324,  2.72247686, -2.30580031, -0.27336713]), 'This principal component from the training data has weight of', -0.34)\n"
     ]
    }
   ],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T \n",
    "variable_names = X_train\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'This principal component from the training data has weight of', round(coef[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb28de10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0])\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, it is important to note that the components are numbered from 0 to 3, although we will refer to them as: Component 1 (0), Component 2 (1), Component 3 (2) and Component 4 (3). \n",
    "\n",
    "From the weights and the plot it is evident that the first 3 components are not as \"heavy\"--with weights of 0.17, 0.11, 0.06, respectively--, as the fourth one that with a weight of -0.38 has the strongest negative correlation. Component 3, on the other hand, even though is positive displays the weakest correlation at 0.06.\n",
    "\n",
    "With this numbers, we dare to affirm that Component 4 is the most important of them all, followed by Component 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Hour as Response Variable - weights\n",
    "\n",
    "In order to determine which principal component is the most important when using the response variabe **HR** or **y_task2**, we again decided to calculate the weights of each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1.71686021,  1.13674501,  2.78865833, -0.61995132]), 'This principal component from the training data has weight of', 0.01)\n",
      "(array([-2.72108962,  0.2404349 ,  1.79932863, -0.56378588]), 'This principal component from the training data has weight of', 0.02)\n",
      "(array([ 2.54187449, -3.82136085, -0.03280968,  0.05496253]), 'This principal component from the training data has weight of', 0.08)\n"
     ]
    }
   ],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf2.coef_.T \n",
    "variable_names = X_train\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'This principal component from the training data has weight of', round(coef[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+AHAMAQ/ABjm/wAfxuMOtuH/0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaeaf518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf2.coef_[0])\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same case as when we did the weights for **FATALITIES** where components are numbered from 0 to 3, and we will refer to them as: Component 1 (0), Component 2 (1), Component 3 (2) and Component 4 (3). \n",
    "\n",
    "In the case of the response **HR** or **y_task2** Component 3 shows the strongest positive correlation with a weight of 0.09. We could say that this component is the most important one, based only on its weight compared to the weights of the other components, but still it shows a weak positive correlation, which makes it difficult for us to determine its real importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - How useful is your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic regression model (the one which is more accurate) could be used to predict if an accident would result in more than one fatality; However, all of the data reported in the FARS dataset are only those accidents that resulted in a death.  Therefore, it may not be as useful as examining more specific details about an accident to determine the severity of injuries or the conditions that allowed someone to survive in the same or similar circumstances.\n",
    "\n",
    "Nevertheless, considering that the attributes that we selected for this analysis are deeply related to the causes and the circumstances surrounding an accident where a fatality or fatalities are present--such as the type weather, the lighting conditions, the type of road, if occurred at an intersection, a main artery, the hour range, etc.-- we can say that the model could be useful for:\n",
    "\n",
    "Law enforcement and cities: to determine where and when most accidents where fatalities occur happen and therefore, allocate more personnel (whether officers or rescue) to those; allocate resources to place better sign postings, install new street lights, etc. to prevent people from dying in a car accident in places and at times where is recurrent. Also to create awareness campaings and PSA's (in the mdeia, traiditonal and digital) to alert people of the conditions, times and places where these kind of events tend to happen more frequently and provide the public with ways to protect themselves at certain times, certain types of roads and weather and lighting conditions, among others, so that they won't become an statistic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - How would you measure the model's value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model's value isn't as valuable after the fact.  More data needs to be collected in order to provide a more educated and informed prediction such as the car type, the age and gender of the driver, the number of vehicle occupants--their ages, genders, etc.--,among others, that will provide with high accuracy, the relation of the elements that can be helpful not only to predict a fatal accident but to prevent it. This will be the kind of prediction that would be most useful for any insurance company, law enforcement, car makers, cities and the public in general.\n",
    "\n",
    "The value of the logistic regression model allows for the prediction that an accident with specific components would result in either a single or multiple fatalities, but again, the reported data only takes into account accidents where fatalities were involved so we cannot compare if a particular intersection or highway or a certain type of weather or lighting condition had more people death than surviving and therefore, we cannot determine the reasons why someone did or did not survive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - How would you deploy your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model could be deployed through a website and the parameters could be interactive. It could allow filtering by geographic area by adding simple code to the data frame manipulation script. A comparison tool could also be implemented when the data is updated in order to compared trends of root causes of fatal traffic accidents over time and illustrate changes in methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - What other data should be collected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we already mentioned before some additional attributes that should be collected would be safety features of the automobiles involved in the accident, such as make, model, and year as well as human attributes such as the drivers' age and gender may also be good data that would make for data that could be added to the model. \n",
    "\n",
    "In addition to the data collected through FARS, additional data could be captured such as non-fatal accidents.  Understading what attributes result in one or more fatalities, is just as important as understading what attributes of an accident did not result in a fatality.  Having all of the data for all accidents for a particular area would allow for analysis to predict what attributes make up a \"perfect storm\" that would contribute to a fatality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - How often would the model need to be updated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model would need to be updated as frequently as new data attributes could be added to the data set.  For example, safety features of automobiles are continuing to evolve. Same if new transit regulations are enforced, such as the prohibition of cell phone usage while driving, from which the data set has no information yet. It would be interesting to know how many of those fatalities were the result of that. Therefore, the model would need to be flexible when these types of changes happen. \n",
    "\n",
    "Most of the data in the FARS dataset is not as susceptible to trends so if the model is to be used on the data as it is reported and captured for FARS, then the model wouldn't need to updated as often; hence it wouldn't be as valuable either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an extension of this project, we will look at predictors of passenger deaths. This will be when fatalities are greater than the number of vehicles involved and excluding incidents involving pedestrians. Because this metric is not broken out, our approximation guarantees that at least one of the fatalities was a passenger, though other accidents may have resulted in a passenger death as well. This approximation yields 888 incidents of about 23,000 or 4%. This analysis will most likely suffer from the same issue as the previous ones with such an unbalanced response class. We will explore how Naive Bayes performs in such a situation compared to a Random Forest. Naive Bayes is considered quite weak when faced with unbalanced classes versus a strong technique in Random Forest.\n",
    "\n",
    "First, we will manipulate our data set to give us what we need in order to train for this particular endeavor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of rows is       ', 28953)\n",
      "('The number of attributes is ', 104)\n",
      "Dimensions of Data Frame excluding Pedestrian Incidents\n",
      "('The number of rows is       ', 23383)\n",
      "('The number of attributes is ', 104)\n",
      "Dimensions of Data Frame with Passenger Fatalities\n",
      "('The number of rows is       ', 23383)\n",
      "('The number of attributes is ', 104)\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_passenger.shape[0])\n",
    "print ('The number of attributes is ', df_passenger.shape[1])\n",
    "\n",
    "df_passenger = df_passenger[df_passenger.PEDS == 0]\n",
    "print ('Dimensions of Data Frame excluding Pedestrian Incidents')\n",
    "print ('The number of rows is       ', df_passenger.shape[0])\n",
    "print ('The number of attributes is ', df_passenger.shape[1])\n",
    "\n",
    "df_passenger['PASS_FATAL'] = 0\n",
    "df_passenger['PASS_FATAL'][df_passenger['FATALS'] > df_passenger['VE_FORMS']] = 1\n",
    "if 'FATALS' in df_passenger:\n",
    "    del df_passenger['FATALS'] \n",
    "print ('Dimensions of Data Frame with Passenger Fatalities')\n",
    "print ('The number of rows is       ', df_passenger.shape[0])\n",
    "print ('The number of attributes is ', df_passenger.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Numer of positive observations: ', 888)\n",
      "('Pipeline accuracy', 0.96202369242612151)\n",
      "('confusion matrix\\n', array([[22495,     0],\n",
      "       [  888,     0]]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEbCAYAAAAvc3j1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niSFBREQ6/T9KT+pynzC3cgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaffe400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "def per_class_accuracy(ytrue,yhat):\n",
    "    conf = mt.confusion_matrix(ytrue,yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue,yhat, title=''):\n",
    "    acc_list = per_class_accuracy(ytrue,yhat)\n",
    "    plt.bar(range(len(acc_list)), acc_list)\n",
    "    plt.xlabel('Class value')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title+\", Total Acc=%.1f\"%(100*mt.accuracy_score(ytrue,yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "\n",
    "positive = df_passenger['PASS_FATAL'].sum()\n",
    "print ('Numer of positive observations: ', positive)\n",
    "    \n",
    "y = df_passenger['PASS_FATAL'].values\n",
    "del df_passenger['PASS_FATAL']\n",
    "X = df_passenger.values    \n",
    "    \n",
    "pca = PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
    "   svd_solver='auto', tol=0.0, whiten=False)\n",
    "X_pca = pca.fit(X).transform(X) # Create a matrix X that contains the  first 5 principal components of the previous X (df_arranged.values)\n",
    "scree_var = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "plt.plot(scree_var)\n",
    "plt.title('Scree Plot',fontsize = 18)\n",
    "plt.xlabel('Principal Components', fontsize=14)\n",
    "plt.ylabel('% Variance Explained', fontsize=14)\n",
    "pca.get_covariance() \n",
    "    \n",
    "cv = StratifiedKFold(n_splits=100)\n",
    "yhat = np.zeros(y.shape)\n",
    "\n",
    "# setup pipeline to take PCA, then fit a different classifier\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',PCA(n_components=3,svd_solver='auto')),\n",
    "     ('CLF',GaussianNB())]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    " \n",
    "conf = mt.confusion_matrix(y,yhat)    \n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('Pipeline accuracy', total_accuracy)\n",
    "print(\"confusion matrix\\n\",conf) \n",
    "plot_class_acc(y,yhat,title=\"Naive Bayes + PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier on the PCA components did not perform well. The 96% accuracy belies the bias in the data set with only 888 positives of 23,000 observations (4%). The classifier clearly labeled every observation as negative, hence the 100% accuracy for the negative class and 0% accuracy for the positive class on the confusion matrix and chart above. This is not surprising as our classes are very uneven and is one of the weaknesses of Naive Bayes since it assesses the probability of each predictor independently based on its probability. Any given predictor is almost guaranteed to be \"hot\" in a negative obeservation with 23,000 observations, and only strong correlations will be able to overcome the scale difference. For example, if one predictor is true in only 5% of the negative values and 100% of the postive values, it will still have a negative probability of 56% (1,150/2,038). This makes Naive Bayes especially weak to unbalanced classes of this magnitude.\n",
    "\n",
    "Let's see if Random Forest performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('confusion matrix\\n', array([[22325,   170],\n",
      "       [  848,    40]]))\n",
      "('RF accuracy', 0.95646409784886455)\n"
     ]
    }
   ],
   "source": [
    "# get a handle to the classifier object, which defines the type\n",
    "clf_RF = RandomForestClassifier(n_estimators = 50\n",
    "                                ,class_weight ='balanced')\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "# NOTE: you can parallelize this using the cross_val_predict method\n",
    "for train, test in cv_object.split(X,y):\n",
    "    clf_RF.fit(X[train],y[train])\n",
    "    yhat[test] = clf_RF.predict(X[test])\n",
    "\n",
    "conf = mt.confusion_matrix(y,yhat)    \n",
    "total_accuracy = mt.accuracy_score(y, yhat) \n",
    "print(\"confusion matrix\\n\",conf) \n",
    "print ('RF accuracy', total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier fared marginally better, but still falls victim to the unbalanced classes, even when setting the class_weight parameter to balanced. In this case, the postive signal in each component may be drowned out by the noise created by the sheer volume of negative responses and their respective variance. \n",
    "\n",
    "The conclusion can be drawn at this point that main contributors to this difference may lie outside of the scope of the collected information or that more data will need to be collected in order to clearly define what predicts passenger deaths under these conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
