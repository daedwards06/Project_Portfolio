{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>MINI - PROJECT </b></h2></center><center><h3><b>MSDS 6371 - 403</b></h3></center><center><h3><b>Data Mining</b></h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Team Members:</b></h4>\n",
    "<ul>\n",
    "  <li>Lisa Mendez</li>\n",
    "  <li>Brandon Lawrence</li>\n",
    "  <li>Dominique Edwards</li>\n",
    "  <li>Mariana Llamas-Cendon</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b><center> SVM and Logistic Regression Modeling</center></b></h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>[50 points]</b>\n",
    "\n",
    "<i>Create a logistic regression model and a support vector machine model for the\n",
    "classification task involved with your dataset. Assess how well each model performs (use\n",
    "80/20 training/testing split for your data).</i>\n",
    "<i><b>Adjust parameters of the models to make them\n",
    "more accurate.</b></i>\n",
    "\n",
    "<i>If your dataset size requires the use of stochastic gradient descent, then\n",
    "linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic\n",
    "regression and linear support vector machines. For many problems, SGD will be required in\n",
    "order to train the SVM model in a reasonable timeframe.</i></h3> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this project we are using the cleaned version of the data set ('accident.xlsx') we have used for Project1, in which those attributes that we consider not useful for our analysis were already removed during Project1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "#import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>ST_CASE</th>\n",
       "      <th>VE_TOTAL</th>\n",
       "      <th>VE_FORMS</th>\n",
       "      <th>PVH_INVL</th>\n",
       "      <th>PEDS</th>\n",
       "      <th>PERNOTMVIT</th>\n",
       "      <th>PERMVIT</th>\n",
       "      <th>PERSONS</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>...</th>\n",
       "      <th>RELJCT1</th>\n",
       "      <th>RELJCT2</th>\n",
       "      <th>TYP_INT</th>\n",
       "      <th>WRK_ZONE</th>\n",
       "      <th>REL_ROAD</th>\n",
       "      <th>LGT_COND</th>\n",
       "      <th>WEATHER</th>\n",
       "      <th>FATALS</th>\n",
       "      <th>DRUNK_DR</th>\n",
       "      <th>DATETIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>127.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10002</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10004</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10005</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  ST_CASE  VE_TOTAL  VE_FORMS  PVH_INVL  PEDS  PERNOTMVIT  PERMVIT  \\\n",
       "0      1    10001         1         1         0     0           0        1   \n",
       "1      1    10002         1         1         0     0           0        1   \n",
       "2      1    10003         1         1         0     0           0        2   \n",
       "3      1    10004         1         1         0     0           0        1   \n",
       "4      1    10005         2         2         0     0           0        2   \n",
       "\n",
       "   PERSONS  COUNTY    ...      RELJCT1  RELJCT2  TYP_INT  WRK_ZONE  REL_ROAD  \\\n",
       "0        1   127.0    ...          0.0      1.0      1.0         0       4.0   \n",
       "1        1    83.0    ...          0.0      1.0      1.0         0       3.0   \n",
       "2        2    11.0    ...          0.0      1.0      1.0         0       4.0   \n",
       "3        1    45.0    ...          0.0      1.0      1.0         0       4.0   \n",
       "4        2    45.0    ...          0.0      2.0      3.0         0       1.0   \n",
       "\n",
       "   LGT_COND  WEATHER FATALS  DRUNK_DR   DATETIME  \n",
       "0       2.0      1.0      1         1 2015-01-01  \n",
       "1       2.0     10.0      1         0 2015-01-01  \n",
       "2       2.0      1.0      1         1 2015-01-01  \n",
       "3       2.0     10.0      1         1 2015-01-04  \n",
       "4       1.0      1.0      1         0 2015-01-07  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accident = pd.read_excel('accident_clean.xlsx')\n",
    "df_accident.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows is        30828\n",
      "The number of attributes is  33\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_accident.shape[0])\n",
    "print ('The number of attributes is ', df_accident.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the attributes from the dataframe called \"df_accident\" as shown in the previous cell using .info( ) need to be coerced from their current type to an object because due to their nature, which will be explained in the next paragraph they will be used just as a reference but do not have any impact in the analysis.\n",
    "Those attributes are:\n",
    "\n",
    "- **ST_CASE** *--which appears continuous (int64) but refers to a unique identifier for each report of an accident;*\n",
    "- **DAY_WEEK** *--which also appears as continuous, should be a discrete variable with values ranging from 1 (Sunday) to 7 (Saturday) that identifies the day of the week in which a particular accident took place;*\n",
    "- **HOUR** *-- which also appears as continuous, we decided that since we couldn't correctly coerce it into an appropriate time format, it could be helpful as an object with values ranging from 0 to 23 as a reference in case we would need it;*\n",
    "- **COUNTY** *--which appears as a float (float64), represents the identifying codes of each of the counties where an accident took place, therefore it could be more useful as an object. *\n",
    "\n",
    "In the next cell, the aforementioned attributes are coerced, as previously mentioned, from their current types to objects. We haven't yet decided at this point if they are worth keeping for this project or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coerce ST_CASE, COUNTY, DAY_WEEK, HOUR from dtype int64 to object as the values are not numbers but strings\n",
    "df_accident[['STATE', 'ST_CASE', 'COUNTY', 'DAY_WEEK', 'HOUR']] = df_accident[['STATE', 'ST_CASE', 'COUNTY', 'DAY_WEEK', 'HOUR']].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous knowledge--from Project1--we have on the data set, in which missing values were present only in those categorical attributes, will be wise to corroborate that no missing values are present in those continuous variables such as VE_TOTAL, VE_FORMS, PVH_INVL, PEDS, PERNOTMVIT, PERSONS, TWAY_ID, LATITUDE, LONGITUDE, FATALS, DRUNK_DR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATE         False\n",
       "ST_CASE       False\n",
       "VE_TOTAL      False\n",
       "VE_FORMS      False\n",
       "PVH_INVL      False\n",
       "PEDS          False\n",
       "PERNOTMVIT    False\n",
       "PERMVIT       False\n",
       "PERSONS       False\n",
       "COUNTY         True\n",
       "DAY_WEEK      False\n",
       "HOUR           True\n",
       "NHS            True\n",
       "RUR_URB        True\n",
       "FUNC_SYS       True\n",
       "RD_OWNER       True\n",
       "ROUTE          True\n",
       "TWAY_ID       False\n",
       "LATITUDE       True\n",
       "LONGITUD       True\n",
       "SP_JUR         True\n",
       "HARM_EV        True\n",
       "MAN_COLL       True\n",
       "RELJCT1        True\n",
       "RELJCT2        True\n",
       "TYP_INT        True\n",
       "WRK_ZONE      False\n",
       "REL_ROAD       True\n",
       "LGT_COND       True\n",
       "WEATHER        True\n",
       "FATALS        False\n",
       "DRUNK_DR      False\n",
       "DATETIME      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values, we expect those to be in the categorical variables but not the continuous ones\n",
    "df_accident.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the results provided in the above cell, our recollection about the original data was right on track: no continuous variables have missing values, but most of categorical variables, except for WRK_ZONE do contain NaNs.\n",
    "\n",
    "Since according to <a href=\"https://pandas.pydata.org/pandas-docs/stable/missing_data.html\">Pandas documentation</a> for missing values, Pandas treats NaNs as zero and we don't know the reason why those values are missing whether because they are really missing, their value should be zero or they were just wrongly recorded, we decided that the best route of action was to get rid of them. It is important to mention that the decision was also influenced by the fact that some of those categorical attributes are binary or have several levels, therefore it would be arbitrary to convert missing values to 0 or any other value. \n",
    "\n",
    "So in the following two cells, we will get rid of missing values from the whole data set, and then check that the missing values are in fact out. \n",
    "\n",
    "Now, remember that missing values are present only on categorical variables, therefore imputation cannot be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values from data set\n",
    "df_accident.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATE         False\n",
       "ST_CASE       False\n",
       "VE_TOTAL      False\n",
       "VE_FORMS      False\n",
       "PVH_INVL      False\n",
       "PEDS          False\n",
       "PERNOTMVIT    False\n",
       "PERMVIT       False\n",
       "PERSONS       False\n",
       "COUNTY        False\n",
       "DAY_WEEK      False\n",
       "HOUR          False\n",
       "NHS           False\n",
       "RUR_URB       False\n",
       "FUNC_SYS      False\n",
       "RD_OWNER      False\n",
       "ROUTE         False\n",
       "TWAY_ID       False\n",
       "LATITUDE      False\n",
       "LONGITUD      False\n",
       "SP_JUR        False\n",
       "HARM_EV       False\n",
       "MAN_COLL      False\n",
       "RELJCT1       False\n",
       "RELJCT2       False\n",
       "TYP_INT       False\n",
       "WRK_ZONE      False\n",
       "REL_ROAD      False\n",
       "LGT_COND      False\n",
       "WEATHER       False\n",
       "FATALS        False\n",
       "DRUNK_DR      False\n",
       "DATETIME      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that missing values are in fact gone\n",
    "df_accident.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of missing values eliminated was 9034, which accounts for about 1/3 of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows is        21794\n",
      "The number of attributes is  33\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_accident.shape[0])\n",
    "print ('The number of attributes is ', df_accident.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **STATE** attribute, which is a categorical, contained 56 levels (from 1 to 56): each one representing a state plus territories such as: \n",
    "- *American Samoa*\n",
    "- *D.C*\n",
    "- *Guam*\n",
    "- *Puerto Rico*\n",
    "- *Virgin Islands*\n",
    "\n",
    "We decided to keep our analysis within the continental USA, including Hawaii and Alaska, and deleting those observations related to the rest of the aforementioned territories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STATE attribute has 56 values, since it includes American Samoa, Puerto Rico, Guam, D.C. and the Virgin Islands. \n",
    "# In this data set we'll focus on only continental USA including Alaska and Hawaii.\n",
    "df_accident = df_accident[df_accident.STATE != 3] # Delete rows that pertain to American Samoa\n",
    "df_accident = df_accident[df_accident.STATE != 11] # Delete rows that pertian to D.C.\n",
    "df_accident = df_accident[df_accident.STATE != 14] # Delete rows that contain Guam\n",
    "df_accident = df_accident[df_accident.STATE != 43] # Delete rows that pertain to Puerto Rico\n",
    "df_accident = df_accident[df_accident.STATE != 52] # Delete rows that pertain to Virgin Islands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an unknown reason, the index of the whole data set was set to start at 0 as opposed to 1, so it was necessary for our own understanding to switch it back to starting at 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change index to start at 1 not 0\n",
    "df_accident.index = np.arange(1, len(df_accident)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting the territories slightly reduced the number of observation in our data set from 21794 to 21773. Only 21 instances were removed. The fact that so few instances were removed indicated that deleting the territories from the dataframe was a good choice because they were not representative of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows is        21773\n",
      "The number of attributes is  33\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_accident.shape[0])\n",
    "print ('The number of attributes is ', df_accident.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that **STATE** attribute still contains 51 levels, turning it into a dummy variable will unnecesarily complicate its interpretation. Therefore, instead we decided to create a new attribute named **REGION** that will separate the values from **STATE** into four regions: West (1), Midwest (2), South (3) and Northeast (4), following how they are divided in <a href=\"https://en.wikipedia.org/wiki/List_of_regions_of_the_United_States\">Wikipedia.</a>\n",
    "This way we can still use **STATE** as a reference or include it in other analysis later on, whereas interpreting **REGION** will not turn out to be a complicated task, and will enable us to discover and create new relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decided to create a REGION attribute considering that the STATE attribute had 49 levels, which we will use only\n",
    "# as a reference, whereas REGION can allow us to interpret, discover and create new relationships. \n",
    "# We decided to group states in only four regions: West (1), Midwest (2), South (3) and Northeast (4), as it is done in\n",
    "# https://en.wikipedia.org/wiki/List_of_regions_of_the_United_States. \n",
    "df_accident['REGION'] = 0\n",
    "# Add states according to the region where they belong:\n",
    "df_accident['REGION'][df_accident['STATE'] == 1] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 2] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 4] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 5] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 6] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 8] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 9] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 10] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 12] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 13] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 15] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 16] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 17] = 2\n",
    "df_accident['REGION'][df_accident['STATE'] == 18] = 2\n",
    "df_accident['REGION'][df_accident['STATE'] == 19] = 2\n",
    "df_accident['REGION'][df_accident['STATE'] == 20] = 2\n",
    "df_accident['REGION'][df_accident['STATE'] == 21] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 22] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 23] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 24] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 25] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 26] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 27] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 28] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 29] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 30] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 31] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 32] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 33] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 34] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 35] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 36] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 37] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 38] = 2\n",
    "df_accident['REGION'][df_accident['STATE'] == 39] = 2\n",
    "df_accident['REGION'][df_accident['STATE'] == 40] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 41] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 42] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 44] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 45] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 46] = 2\n",
    "df_accident['REGION'][df_accident['STATE'] == 47] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 48] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 49] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 50] = 4\n",
    "df_accident['REGION'][df_accident['STATE'] == 51] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 53] = 1\n",
    "df_accident['REGION'][df_accident['STATE'] == 54] = 3\n",
    "df_accident['REGION'][df_accident['STATE'] == 55] = 2\n",
    "df_accident['REGION'][df_accident['STATE'] == 56] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is imperative to corroborate that the new attribute **REGION** was created and it has the correct levels. The number of attributes after creating **REGION** should be 34. The levels of **REGION** should range from 1-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    9746\n",
       "4    4454\n",
       "2    3951\n",
       "1    3622\n",
       "Name: REGION, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accident.REGION.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows is        21773\n",
      "The number of attributes is  34\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_accident.shape[0])\n",
    "print ('The number of attributes is ', df_accident.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use one-hot encoding to turn categorical variables with several levels to dummies. This will not affect binary ones, but we expect a dramatic increase in the number of attributes considering that some of those categoricals have more than 20 levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn categorical variables into dummy ones. \n",
    "# Create dummy variable from RUR_URB attribute\n",
    "rur_df = pd.get_dummies(df_accident.RUR_URB,prefix='RUR_URB')\n",
    "df_accident = pd.concat((df_accident, rur_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from FUNC_SYS attribute\n",
    "func_df = pd.get_dummies(df_accident.FUNC_SYS,prefix='FUNC_SYS')\n",
    "df_accident = pd.concat((df_accident, func_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from RD_OWNER attribute\n",
    "rdowner_df = pd.get_dummies(df_accident.RD_OWNER,prefix='RD_OWNER')\n",
    "df_accident = pd.concat((df_accident, rdowner_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from ROUTE attribute\n",
    "route_df = pd.get_dummies(df_accident.ROUTE,prefix='ROUTE')\n",
    "df_accident = pd.concat((df_accident, route_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from SP_JUR attribute\n",
    "spjur_df = pd.get_dummies(df_accident.SP_JUR,prefix='SP_JUR')\n",
    "df_accident = pd.concat((df_accident, spjur_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from HARM_EV attribute\n",
    "harm_df = pd.get_dummies(df_accident.HARM_EV,prefix='HARM_EV')\n",
    "df_accident = pd.concat((df_accident, harm_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from RELJCT2 attribute\n",
    "reljct_df = pd.get_dummies(df_accident.RELJCT2,prefix='RELJCT2')\n",
    "df_accident = pd.concat((df_accident, reljct_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from TYP_INT attribute\n",
    "typint_df = pd.get_dummies(df_accident.TYP_INT,prefix='TYP_INT')\n",
    "df_accident = pd.concat((df_accident, typint_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from WRK_ZONE attribute\n",
    "wrk_df = pd.get_dummies(df_accident.WRK_ZONE,prefix='WRK_ZONE')\n",
    "df_accident = pd.concat((df_accident, wrk_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from REL_ROAD attribute\n",
    "relrd_df = pd.get_dummies(df_accident.REL_ROAD,prefix='REL_ROAD')\n",
    "df_accident = pd.concat((df_accident, relrd_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from LGT_COND attribute\n",
    "lgt_df = pd.get_dummies(df_accident.LGT_COND,prefix='LGT_COND')\n",
    "df_accident = pd.concat((df_accident, lgt_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from WEATHER attribute\n",
    "wtr_df = pd.get_dummies(df_accident.WEATHER,prefix='WEATHER')\n",
    "df_accident = pd.concat((df_accident, wtr_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy variable from REGION attribute\n",
    "region_df = pd.get_dummies(df_accident.REGION,prefix='REGION')\n",
    "df_accident = pd.concat((df_accident, region_df), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variable from DAY_WEEK attribute\n",
    "wk_df = pd.get_dummies(df_accident.DAY_WEEK,prefix='DAY_WEEK')\n",
    "df_accident = pd.concat((df_accident, wk_df), axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the one-hot encoding of the above attributes, we delete those variables from which they were derived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To clean df_accident, drop those attributes from which the dummy variables were derived\n",
    "\n",
    "if 'RUR_URB' in df_accident:\n",
    "    del df_accident['RUR_URB'] \n",
    "    \n",
    "if 'FUNC_SYS' in df_accident:\n",
    "    del df_accident['FUNC_SYS'] \n",
    "    \n",
    "if 'RD_OWNER' in df_accident:\n",
    "    del df_accident['RD_OWNER'] \n",
    "    \n",
    "if 'ROUTE' in df_accident:\n",
    "    del df_accident['ROUTE'] \n",
    "    \n",
    "if 'SP_JUR' in df_accident:\n",
    "    del df_accident['SP_JUR'] \n",
    "    \n",
    "if 'HARM_EV' in df_accident:\n",
    "    del df_accident['HARM_EV'] \n",
    "    \n",
    "if 'RELJCT2' in df_accident:\n",
    "    del df_accident['RELJCT2'] \n",
    "    \n",
    "if 'TYP_INT' in df_accident:\n",
    "    del df_accident['TYP_INT'] \n",
    "    \n",
    "if 'WRK_ZONE' in df_accident:\n",
    "    del df_accident['WRK_ZONE'] \n",
    "    \n",
    "if 'REL_ROAD' in df_accident:\n",
    "    del df_accident['REL_ROAD'] \n",
    "    \n",
    "if 'LGT_COND' in df_accident:\n",
    "    del df_accident['LGT_COND'] \n",
    "    \n",
    "if 'WEATHER' in df_accident:\n",
    "    del df_accident['WEATHER'] \n",
    "    \n",
    "if 'REGION' in df_accident:\n",
    "    del df_accident['REGION']\n",
    "\n",
    "if 'DAY_WEEK' in df_accident:\n",
    "    del df_accident['DAY_WEEK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was expected, the number of attributes rose from 34 to 171. The one-hot encoding added 137 attributes to the dataframe \"df_accident\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows is        21773\n",
      "The number of attributes is  171\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_accident.shape[0])\n",
    "print ('The number of attributes is ', df_accident.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the number of attributes from the dataframe, we decided to create a new dataframe named \"df_arranged\", which will not contain those attributes that we had converted into objects or that will be only used as reference due to the fact that they reference fixed values, specific characteristics or unique identifiers that we will not consider for this analysis. \n",
    "\n",
    "We expect the number of attributes from 171 to 166, considering that only five attributes will be dropped. The reduction isn't dramatic but since our response variable **FATALS**--which refers to the number of people death during an accident--is a continuous one, we expect to derive from it a new binary attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that does not contain attributes which type is object or datetime as those are only used \n",
    "# as references and will not be considered in the analysis because their values reflect specific characteristics \n",
    "# that are not either continuous nor categorical, such as STATE, in which each state is defined instead of its \n",
    "# name by a number but could be helpful later on to stablish relationships. \n",
    "\n",
    "df_arranged = df_accident\n",
    "df_arranged.drop('STATE', axis= 1, inplace =True) # Unique identifier of Sate by number\n",
    "df_arranged.drop('ST_CASE', axis= 1, inplace =True) # Unique identifier of accident report case\n",
    "df_arranged.drop('COUNTY', axis= 1, inplace =True) # Unique identifier of the County where the accident occur\n",
    "df_arranged.drop('DATETIME', axis= 1, inplace =True) \n",
    "df_arranged.drop('TWAY_ID', axis= 1, inplace =True) # Names of the arteries where an accident happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows is        21773\n",
      "The number of attributes is  165\n"
     ]
    }
   ],
   "source": [
    "print ('The number of rows is       ', df_arranged.shape[0])\n",
    "print ('The number of attributes is ', df_arranged.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, a new binary variable that will represent the response variable with values of 0 and 1 need to be created under the name **FATALITIES** since it will be derived from the attribute **FATALS** which is a continuous one, and will be dropped after **FATALITIES** has been created.\n",
    "\n",
    "For this purpose, the new binary attribute **FATALITIES** will be splitted in two values (0 and 1): 0, representing the number of single deaths in an accident, and 1 corresponding to those accidents where multiple deaths occurred. \n",
    "\n",
    "Right after, the attribute **FATALS** from which **FATALITIES** was derived will be deleted from dataframe \"df_arranged\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20146\n",
       "1     1627\n",
       "Name: FATALITIES, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since our chosen response variable FATALS is continuous with values ranging from 1-99, transform it into a new binary attribute 'FATALITIES'\n",
    "# Where 0 = single fatality and 1 = multiple fatalities. \n",
    "df_arranged['FATALITIES'] = 0\n",
    "df_arranged['FATALITIES'][df_arranged['FATALS'] == 1] = 0\n",
    "df_arranged['FATALITIES'][df_arranged['FATALS'] > 1] = 1\n",
    "df_arranged['FATALITIES'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the original response variable from the dataframe so it won't influence results. \n",
    "if 'FATALS' in df_arranged:\n",
    "    del df_arranged['FATALS'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will divide our dataframe into numpy matrices corresponding to our response variable  **FATALITIES** which will be now labeled as \"y\" and the rest of the data as explanatory \"X\", which will be used to predict. Then, it will be safe to remove the class label  **FATALITIES** from the dataframe \"df_arranged\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# first we define which attribute will be our response one (y), and define which will be the explanatory variables \n",
    "# that will make up our model, and we create matrices of each one. \n",
    "\n",
    "if 'FATALITIES' in df_arranged:\n",
    "    y = df_arranged['FATALITIES'].values\n",
    "    del df_arranged['FATALITIES']\n",
    "    X = df_arranged.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that even after dividing the dataframe into y (response) and X (explanatory), our X still has a considerable dimension of at least 165 attributes (most of them dummy variables which are now binary), a dimension reduction using PCA will be quite useful since binary variables do not represent a problem and it will facilitate a better interpretation of the data and the results from the analysis.\n",
    "\n",
    "Our data does not need to be standardized before running a PCA because all the continuous variables are on the same scale. \n",
    "\n",
    "We first decided to choose only 5 components for this task but (see image below) the results were overwhelming considering that the scree plot for those five components displayed that the first principal component (0) explained 65% of the variance, followed by the second (1) explaining 85%; the third (2) explaining 95%,  the fourth (3) explaining about 98% and the last one (4), a little over 100%. Based on this results seems that the fifth principal component seems to be redundant in relation to the rest. \n",
    "\n",
    "<img src='screeplot_5comp.png'>\n",
    "\n",
    "Therefore, to avoid such situation four components were chosen instead as shown in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.35637353e+01,   1.22279110e-03,  -2.32365421e-04, ...,\n",
       "          1.27938617e-04,   2.54532056e-04,  -1.60387793e-04],\n",
       "       [  1.22279110e-03,   1.35642994e+01,  -3.31720807e-04, ...,\n",
       "          1.88056834e-04,   3.80683116e-04,  -2.61885060e-04],\n",
       "       [ -2.32365421e-04,  -3.31720807e-04,   1.35628442e+01, ...,\n",
       "         -6.01182164e-05,  -1.26151060e-04,   1.01497267e-04],\n",
       "       ..., \n",
       "       [  1.27938617e-04,   1.88056834e-04,  -6.01182164e-05, ...,\n",
       "          1.35627821e+01,   7.61582041e-05,  -6.28424479e-05],\n",
       "       [  2.54532056e-04,   3.80683116e-04,  -1.26151060e-04, ...,\n",
       "          7.61582041e-05,   1.35629125e+01,  -1.38018553e-04],\n",
       "       [ -1.60387793e-04,  -2.61885060e-04,   1.01497267e-04, ...,\n",
       "         -6.28424479e-05,  -1.38018553e-04,   1.35628631e+01]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEeCAYAAACOtbLLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b21d8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Our dataframe is quite big: With 171 attributes and a little over 21,000 observations we need to reduce dimensionality.\n",
    "# For this task, we will use PCA for our explanatory variables, excluding our response variable. \n",
    "# We will use pca.transform to create a new X that contains only 5 principal components, and which will be used in \n",
    "# logistic regression.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(copy=True, iterated_power='auto', n_components=4, random_state=None,\n",
    "   svd_solver='auto', tol=0.0, whiten=False)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X) # Create a matrix X that contains the  first 5 principal components of the previous X (df_arranged.values)\n",
    "scree_var = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "plt.plot(scree_var)\n",
    "plt.title('Scree Plot',fontsize = 18)\n",
    "plt.xlabel('Principal Components', fontsize=14)\n",
    "plt.ylabel('% Variance Explained', fontsize=14)\n",
    "pca.get_covariance() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the scree plot, the first principal component (0) explains 67% of the variance, followed by the second (1) explaining 82%; the third (2) explaining 92%, while the fourth (3) explains about 95%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Training and Testing Split</b></h3>\n",
    "\n",
    "After reducing the dimensions of our explanatory variables (X) data into four principal components (X_pca), we will split the data into a training set, containing 80% of the data, and a test set containing the remaining 20%. Since we would like to make sure that the subgroups of data that will be split into training and testing are not only random but proportional.\n",
    "\n",
    "We will choose a cross-validation with 10 folds as it will provide better estimate of the error, and it will helps us determine the predictive power of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=10, random_state=None, test_size=0.2,\n",
      "            train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# Set cross-validation parameters, which is what we will use to determine the predictve power of our model.\n",
    "num_cv_iterations = 10 # we ara choosing 10 folds because it provides the best estimate of error.\n",
    "num_instances = len(y)\n",
    "\n",
    "# Now split data into Training (80%) and Test (20%) data sets\n",
    "\n",
    "cv_object = StratifiedShuffleSplit(n_splits = num_cv_iterations, test_size = 0.2) # we use stratified to make sure that \n",
    "# train and test data contains important information on both\n",
    "\n",
    "print(cv_object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Logistic Regression </b></h3>\n",
    "\n",
    "By creating a logistic regression object named \"lr_clf\" we will be able to fit our training data, which will be now strored in new variables: X_train containing our four chosen components as the explanatory variable and y_train containing the response variable, and use the test data, also contained in new variables: X_test (explanatory) and y_test (response), which we will use to predict by creating an object called y_hat.\n",
    "\n",
    "Also, we will use the test response variable y_test and the object y_hat to calculate the accuracy score and the confusion matrices based on the 10 folds that we selected for the cross validation object called cv_object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.924225028703\n",
      "confusion matrix\n",
      " [[4021    9]\n",
      " [ 321    4]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.925373134328\n",
      "confusion matrix\n",
      " [[4026    4]\n",
      " [ 321    4]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.922847301952\n",
      "confusion matrix\n",
      " [[4018   12]\n",
      " [ 324    1]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.923765786452\n",
      "confusion matrix\n",
      " [[4020   10]\n",
      " [ 322    3]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.923536165327\n",
      "confusion matrix\n",
      " [[4018   12]\n",
      " [ 321    4]]\n",
      "====Iteration 5  ====\n",
      "accuracy 0.923536165327\n",
      "confusion matrix\n",
      " [[4019   11]\n",
      " [ 322    3]]\n",
      "====Iteration 6  ====\n",
      "accuracy 0.924684270953\n",
      "confusion matrix\n",
      " [[4024    6]\n",
      " [ 322    3]]\n",
      "====Iteration 7  ====\n",
      "accuracy 0.924684270953\n",
      "confusion matrix\n",
      " [[4022    8]\n",
      " [ 320    5]]\n",
      "====Iteration 8  ====\n",
      "accuracy 0.924454649828\n",
      "confusion matrix\n",
      " [[4025    5]\n",
      " [ 324    1]]\n",
      "====Iteration 9  ====\n",
      "accuracy 0.923765786452\n",
      "confusion matrix\n",
      " [[4021    9]\n",
      " [ 323    2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# Create logistic regrssion object\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) #We will start with a ost of 1.0 and then compare\n",
    "# with a model with a lower cost later on\n",
    "\n",
    "#We decided to use this code from notebook 04 because it is clear what it does, and how it does it. \n",
    "iter_num=0 \n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    lr_clf.fit(X_train,y_train)  # here we use our train data to fit the logistic regression\n",
    "    y_hat = lr_clf.predict(X_test) # and we use test data to predict\n",
    "\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The accuracy of the 10 different models ranges from 92.1% - 92.4%, which is to be expected considering that our response variable is unbalanced with 92.5% (20,146 instances) of the data corresponding to class 0 and only 7.5% (1,627 instances) corresponding to class 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Parameters changed in model </b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=100.00\n",
      "Sparsity with L1 penalty: 0.00%\n",
      "score with L1 penalty: 0.9246\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.9246\n",
      "====Iteration 10  ====\n",
      "accuracy 0.923765786452\n",
      "confusion matrix\n",
      " [[4021    9]\n",
      " [ 323    2]]\n",
      "====Iteration 10  ====\n",
      "accuracy 0.923765786452\n",
      "confusion matrix\n",
      " [[4021    9]\n",
      " [ 323    2]]\n",
      "C=1.00\n",
      "Sparsity with L1 penalty: 0.00%\n",
      "score with L1 penalty: 0.9246\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.9246\n",
      "====Iteration 10  ====\n",
      "accuracy 0.923765786452\n",
      "confusion matrix\n",
      " [[4021    9]\n",
      " [ 323    2]]\n",
      "====Iteration 10  ====\n",
      "accuracy 0.923765786452\n",
      "confusion matrix\n",
      " [[4021    9]\n",
      " [ 323    2]]\n",
      "C=0.01\n",
      "Sparsity with L1 penalty: 25.00%\n",
      "score with L1 penalty: 0.9247\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.9246\n",
      "====Iteration 10  ====\n",
      "accuracy 0.923765786452\n",
      "confusion matrix\n",
      " [[4021    9]\n",
      " [ 323    2]]\n",
      "====Iteration 10  ====\n",
      "accuracy 0.923765786452\n",
      "confusion matrix\n",
      " [[4021    9]\n",
      " [ 323    2]]\n"
     ]
    }
   ],
   "source": [
    "#Set regularization parameter\n",
    "for i, C in enumerate((100, 1, 0.01)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    clf_l1_LR.fit(X_train,y_train)\n",
    "    clf_l2_LR.fit(X_train,y_train)\n",
    "\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n",
    "    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(X_train,y_train))\n",
    "    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n",
    "    print(\"score with L2 penalty: %.4f\" % clf_l2_LR.score(X_train,y_train))\n",
    "    \n",
    "    y_hat_l1 = clf_l1_LR.predict(X_test)\n",
    "    y_hat_l2 = clf_l2_LR.predict(X_test)\n",
    "    \n",
    "    acc = mt.accuracy_score(y_test,y_hat_l1)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat_l1)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "    acc = mt.accuracy_score(y_test,y_hat_l2)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat_l2)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Switching to the l2 penalty yields greater sparsity without a reduction in the predicitve power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.92330654  0.92491389  0.92514351  0.92399541  0.9228473   0.92537313\n",
      "  0.9228473   0.92468427  0.92376579  0.92422503]\n"
     ]
    }
   ],
   "source": [
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None) # get object\n",
    "    accuracies = cross_val_score(lr_clf,X,y=y,cv=cv_object) # this also can help with parallelism\n",
    "    print(accuracies)\n",
    "\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b> Suppor Vector Machine </b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.925373134328\n",
      "[[4030    0]\n",
      " [ 325    0]]\n",
      "(4114, 4)\n",
      "(4114,)\n",
      "[2812 1302]\n"
     ]
    }
   ],
   "source": [
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy not materially improved on our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3><b>[10 points]</b>\n",
    "\n",
    "<i>Discuss the advantages of each model for each classification task. Does one type\n",
    "of model offer superior performance over another in terms of prediction accuracy? In terms of\n",
    "training time or efficiency? Explain in detail.</i>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Understanding the differences between theoretical statistical procedures and SVM helps to determine the advantages and disadvantages, and both are dependent on the type of data to be modeled.  Real-world, statistical methods usually rely on scientifically based statistical procedures (Anguita, Ghio, Greco, Oneto, & Ridella).  These methods split the data into a training dataset and a hold-out set.  The training set it used to create a model, while the hold-out set is used for computing the generalization error estimation (Anguita, Et. Al.).  As stable as these practical methods are, they tend to find a separating hyperplane, but not usually an optimal one (Berwick).\n",
    "\n",
    "SVM MLTâ€™s goal is to maximize the margin around the separating hyperplane (Berwick).  The â€œsupport vectors are the elements of the training set that would change the position of the dividing hyperplane if removedâ€ (Berlick). In a post on Stack Exchange, Gavin Simpson outlines a few advantages and disadvantages of SVM.  One not so significant advantage he points out is a simple one, that SVM has a â€œregularization parameterâ€ that assists the user to avoid over-fitting.  He also believes that since it uses the â€œkernel trickâ€ it can build in expert knowledge about a problem using by engineering the kernel (Simpson).\n",
    "\n",
    "Although, SVM has a learning phase that finds a set of parameters by using quadratic programming, usually the search for the most optimal parameters does not complete and additional tuning must be done (Anguita, Et. Al.).  Using optimization techniques, such as Lagrange multipliers, can aid to get the data into an arrangement that can be solved analytically (Berwick).  Simpson noted one disadvantages is that the SVM theory only covers the â€œdetermination of the parameters for a given value of the regularization and kernel parameters and choice of kernelâ€.\n",
    "\n",
    "There doesnâ€™t seem to be much difference in performance based on the information provided, and we believe it is dependent on the imbalance prevalent on our response variable (y), where the difference in the number of instances between class 0 and class 1 are important considering that class 0 holds about 92.5% of the data. This imbalance definitely influenced the accuracy scores, particularly when there was no cost set for any missclassification of false negatives or positives. Now we know that we could have prevented this by creating a cost matrix in which we can assign lower costs to the true positive, a higher cost to those false negatives, and lower but positive cost to the false positives, and then multiply it by the each of the confusion matrices. That way we could obtain a more reliable and realistic accuracy score regarding the number of people that were reported as dead when they did not die in an accident and vice versa. \n",
    "\n",
    "If we were to base our model decision on the accuracy scores retrieved by either the logistic regression or the SVM, this will be a difficult task because the range of accuracy scores of the logistic regression with 10 fold cross-validation was from 92.1% - 92.4%, while the accuracy retrieved by SVM achieved a 92.5%. There is just 1% difference between the highest end of the range of logistic regression models and SVM, but a 1% could mean a world of difference when it comes to prediction. \n",
    "\n",
    "Now, based on the confusion matrices provided by each of the models, in which SVM returned one where there were no false negatives due to the imbalance of the data which cause a high sensitivity in which no cost was assigned to the false positives, and based on this the logistic regression seem as a more efficient model. \n",
    "\n",
    "\n",
    "In terms of training time and efficiency, SVM seems a much more complicated model because it involves transforming the data and the use of kernels and also because perhaps because we are more familiar with logistic regression which we have seen previously but at the end of the day, if the data is cleaned, and correctly splitted into training and testing sets, both models represent an efficient way to classify data. \n",
    "\n",
    "Although \n",
    "\n",
    "In our case, the dimensionality reduction of the data using PCA represented a challenge regarding the correct interpretation of the data considering that python does not provide a covariance matrix in which we could have detected which attributes had a higher covariance in each of the components.\n",
    "\n",
    "There was more of a difference within the model by choosing one parameter (cost) over another.  We observed that the lower the cost, the higher the accuracy.  This makes sense because C is the â€œpenalty on the slack variables which measure the degree to which the margin constraints are violatedâ€ (Chin).  For our data, there wasnâ€™t a significant difference.  It is more beneficial to look within the regression model and fine tune it rather than use SVM.\n",
    "\n",
    "Our data illustrates what Edwin Chin stated on his blog, â€œbetter data often beats better algorithmsâ€.  That holds true for our data set which is why both models performed well.   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "at  Even though the metric of accuracy does not contemplates the cost of missclassifying false negatives and false positives, both the logistic regression models and the SVM returned an accuracy score that is quite similar\n",
    "\n",
    "Resources:\n",
    "\n",
    "Anguita, D., Ghio, A., Greco, N., Oneto, L., & Ridella, S. Model Selection for Support Vector Machines: Advantages and Disadvantages of the Machine Learning Theory.  Smartlab.  http://www.smartlab.ws/files/pubblications/ICP/AngGhiRid_IJCNN10.pdf.  Accessed June 18, 2017.\n",
    "\n",
    "Berwick, R. An idiotâ€™s guide to Support vector machines (SVMs).  Village Idiot.  http://www.svms.org/tutorials/Berwick2003.pdf.  Accessed June 18, 2017\n",
    "\n",
    "Chin, E.  Edwin Chin Blog.  http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier.  Accessed June 18, 2017.\n",
    "\n",
    "Simpson, G.  Stack Exchange Blog Post.  May 2, 2014.  https://stats.stackexchange.com/questions/24437/advantages-and-disadvantages-of-svm.  Accessed June 18, 2017.\n",
    "\n",
    "Marsupial, D.  Stack Exchange Blog Discussion.  Cross Validated.  â€œSVM: Why does the number of support vectors decrease when C is increased?â€ https://stats.stackexchange.com/questions/270187/svm-why-does-the-number-of-support-vectors-decrease-when-c-is-increased.  Accessed June 18, 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>[30 points]</b>\n",
    "\n",
    "<i>Use the weights from logistic regression to interpret the importance of different\n",
    "features for the classification task. Explain your interpretation in detail. Why do you think\n",
    "some variables are more important?</i></h3>\n",
    "\n",
    "Considering that we reduced the dimensionality of our dataframe \"df_arranged\" using PCA selecting only five principal components that later were used to split data into training and test sets, which in turn were used to pass through the logistic regression object, it seems adequate to calculate the weights of those five components that are now part of the train set (X_train) in order to decide if all of them are worth keeping and if their presence do not cause an overfitting of the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.34418077 -8.09625071 -4.50701903  1.56495696] This principal component from the training data has weight of 0.0650377084484\n",
      "[ 17.45764237  12.41880561   6.60591012   2.22248572] This principal component from the training data has weight of 0.0154477597284\n",
      "[-1.96034614 -7.30178685  5.62585816 -2.64923759] This principal component from the training data has weight of -0.00157791612436\n",
      "[ 29.2995645    1.41928788   6.18967858  -2.69939921] This principal component from the training data has weight of 0.604316844227\n"
     ]
    }
   ],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T \n",
    "variable_names = X_train\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'This principal component from the training data has weight of', coef[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRwHAQIQ7ABiIcAcAAxHuAGCg/wNMtH4qRUTUcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b3aacf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0])\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "  <li>The first component identified with the number 0 has a weight of:  0.0650377084484</li>\n",
    "  <li>The second component identified with the number 1 has a weight of: 0.0154477597284</li>\n",
    "  <li>The third component identified with the number 2 has a weight of: -0.00157791612436</li>\n",
    "  <li>The fourth component identified with the number 3 has a weight of: 0.604316844227</li>\n",
    "</ul>\n",
    "\n",
    "The first two components (0 and 1) and the last one (3)--also the \"heaviest\" one with a weight of 0.604316844227--  are positively correlated. The second component has a negative weight of -0.00157791612436 (for that reason it is not displayed on the graph), which makes it the \"lighest\" one and the only one that is negatively correlated to the rest. (It is beyond our understanding why the plot is only showing positive weight values.)\n",
    "\n",
    "\n",
    "Regarding component 0, which array of values is [-5.34418077 -8.09625071 -4.50701903  1.56495696], we can definitely notice that the first three elements are negatively correlated, being -4.50701903 the highest negative loading, while the fourth element possesses the highest positive loading of 1.56495696.\n",
    "\n",
    "For component 1, which array of values is [ 17.45764237  12.41880561   6.60591012   2.22248572], we can see that all of the elements are positively correlated with the first two elements displaying the highest loadings at 17.45764237 and 12.41880561. \n",
    "\n",
    "For component 2, which array of values is [-1.96034614 -7.30178685  5.62585816 -2.64923759], we see something similar to component 0 where three out four elements are negatively correlated, although in this case the highest negative loading is even greater at -2.64923759 than that of component 0, which was of -4.50701903. The highest positive loading is located on the third element with 5.62585816.\n",
    "\n",
    "Finally, for component 3, which array of values is [ 29.2995645    1.41928788   6.18967858  -2.69939921]. Just by looking at the loadings it is easy to understand why this component is the heaviest one not only because three of its elements are positive while only one is negative, but because the first element displays the highest loading of all of the components with a value of 29.2995645. Definitely this first element is responsible for the total weight of the component itself. \n",
    "\n",
    "Although PCA itself standardizes the data, we would like to see if \"normalizing\" the components of the training set creates a change that could be benefitial by scaling the four components to see if the third, and heaviest, could become more similar in its weight to the rest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.923765786452\n",
      "[[4021    9]\n",
      " [ 323    2]]\n",
      "[-5.34418077 -8.09625071 -4.50701903  1.56495696] This component has a weight of 0.0650377084484\n",
      "[ 17.45764237  12.41880561   6.60591012   2.22248572] This component has a weight of 0.0154477597284\n",
      "[-1.96034614 -7.30178685  5.62585816 -2.64923759] This component has a weight of -0.00157791612436\n",
      "[ 29.2995645    1.41928788   6.18967858  -2.69939921] This component has a weight of 0.604316844227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalization\n",
    "\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) \n",
    "\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) \n",
    "X_test_scaled = scl_obj.transform(X_test) \n",
    "\n",
    "# train the model \n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) \n",
    "lr_clf.fit(X_train_scaled,y_train)  \n",
    "\n",
    "# get test set precitions\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and display results\n",
    "zip_vars = zip(lr_clf.coef_.T,X_train) \n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'This component has a weight of', coef[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRwHAQIQ7ABiIcAcAAxHuAGCg/wNMtH4qRUTUcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d252a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0])\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization did not at all affect the weights of any of the components or the values of their loadings. They remained untouched, after which we can conclude that by far component 3 is the strongest of them all. \n",
    "\n",
    "The only component we would eliminate is component 2 not because it is negatively correlated but because at -0.00157791612436 its correlation is not as strong in comparison to the rest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>[10 points]</b>\n",
    "\n",
    "<i>Look at the chosen support vectors for the classification task. Do these provide\n",
    "any insight into the data? Explain. If you used stochastic gradient descent (and therefore did\n",
    "not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€”\n",
    "then analyze the support vectors from the subsampled dataset.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Due to the accuracy of our prediction, and the fact that we didnâ€™t have too large of a data set, we did not feel we needed to use stochastic gradient descent.  In addition, we were not adding data to our training set, but gradient descent could be used if we were. \n",
    "\n",
    "Looking at the output of SVM, we have 3919 support vectors for the five components (which resulted from PCA); 2617 and 1302 in each class.  These support vectors lie on the margin.  A couple of insights we can glean from this information is that our data is linearly separable with a large margin.  \n",
    "\n",
    "(3919, 5)\n",
    "(3919,)\n",
    "[2617 1302]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {
    "14da91c4fd54443d8b51e0d68e8c0bd1": {
     "views": [
      {
       "cell_index": 66
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
